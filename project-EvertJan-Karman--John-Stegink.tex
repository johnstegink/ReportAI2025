\documentclass[11pt, a4paper]{article}
\usepackage{geometry}
\usepackage[parfill]{parskip}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{enumerate}

\title{Project ``Hessian Matrices and : '' \\
	{Title} \\ 
	IB3702 Mathematics for Machine Learning} % Fill in the chosen topic; and optionally, give a title to your project (you can overwrite {Title})
\author{Evertjan Karman \and John Stegink} % Swap with your names
\date{15 November, 2025} % adjust date: when do you submit this assignment?

\begin{document}
\maketitle

\section{Introduction}
The basis of a machine learning algorithm that it tries to predict the right output using a certain input. First the algorithm will have to be trained using correct data. During the training process, the difference between the predicted value and the actual value must be minimized. A cost function is used to quantize the difference, this difference must be minimized. To find the minimum value, the machine learning algorithm iterates until it has found the minimum value. The methods for this iteration are numerous, the most well known algorithm is gradient descent (sections \ref{GradientDescent1} and \ref{GradientDescent2} ). For the training to be as effective as possible it is necessary to find the minimum in little iterations. The method for finding the minimum that is discussed in this report is the use of Hessian Matrices (section \ref{HessianMatricesSub}). Both gradient descent and the Hessian matrix make use of the Newton method (sections \ref{Newton1} and \ref{Newton2}).
	\\
	\\
	First a description of gradient descent and Newton's will be given for functions using one variable, this is to make the principle clear. Normally for machine learning, 1 variable is not sufficient, the loss function mostly contains multiple variables. The gradient descent, Newton's method and Hessian matrices will be described from the calculus point of view. The linear algebra part will not be discussed (especially eigenvalue and eigenvectors). Reading this report a requires basic understanding of machine learning, especially the proces of machine learning (training and deployment) and the principles of supervised learning.
	\\
	\\
	Using the Hessian Matrix finds the minimum in far less iterations than using gradient descent. The problem of using Hessian Matrices is that calculating a Hessian Matrix is much more complicated and time consuming than using just the derivative as is done using gradient descent.

\section{Preliminaries}

\subsection{Notation}
Used notation:
\begin{itemize}
  \item Multivariable derivates $\delta$ 
\end{itemize}

\subsection{Concepts}
\subsection{Techniques}
\begin{itemize}
  \item 
    Calculating derivative
  \item 
    Calculating second derivatie
  \item 
    Computing eigen values

\end{itemize}

\subsection{Problems}
\subsubsection{Problem 1}
Maken van een op hessian matrix op basis van een functie
\subsubsection{Problem 2}
Text\ldots


\section{Methods}
\subsection{Gradient descent with one variable}
\label{GradientDescent1}
The purpose of using gradient descent is to find a local or global minimum of a differentiable function by iteratively adjusting the parameters in the direction of the steepest descent. This a mouthful. \\It can be clarified by the following metaphor: 	When you want to find the path from the top to the foot of the mountain in the mist. You walk a few meters down the path with the steepest descent. Then you determine the next path with the steepest descent\footnote{https://en.wikipedia.org/wiki/Gradient\_descent}. Appendix \ref{metaGradientDescent} contains a more detailed version of this metaphor.
\\
\\Assume that we have a continuous function f defined on $\mathbb{R}$ (fig \ref{contfunctionR}).This function:
\begin{itemize}
  \item is differentiable with derivative $f'(x)$.
  \item has a starting point $x_0$.
\end{itemize}

Then we get $x_1$ by subtracting $f'(x_0) \cdot \alpha$ from {x_0}, where $\alpha$ is called the learning rate. This is equal to walking the path in the steepest descent by $\alpha$ meters in the metaphore. The value of $\alpha$  will be chosen before starting the procedure. Usual values for $\alpha$ are $0.01$ or $0.05$.\\
\\
We iterate this, so that we get an \textbf{array??} which is recursively defined as:\\
\[x_{k+1} = x_k - f'(x_k) \cdot \alpha\]

This array will converge to the minimum of $f$ (fig \ref{contfunctionR}).
The pitchfalls here are, that the procedure may end in a local minimum, while $f$ has a stronger minimum elsewhere.\\
Or with a less than optimal choice for the learning rate, the array could even diverge .


\subsection{Newton's method with one variable}
\label{Newton1}
Newton's method finds the zeroes of a function $f$, in cases where solving the equation is not possible. It works by iterating through the following steps (inspired by \cite{NetwtonsMethod}:
\begin{enumerate}
  \item Start at a random point on the curve.
  \item Determine the tangent line at that point (using the derivative).
  \item Determine the point where the tangent line hits the X-axis
\item Continue with step 2 until the value does not change significantly anymore. 
\end{enumerate}


\begin{figure}[htbp]
    \begin{minipage}{0.6\textwidth}
	    \includegraphics[width=7.5cm,keepaspectratio]{images/fx_example_01.png}
    \caption{A continuous function $f$ defined on $\mathbb{R}$ }
    \label{contfunctionR}
    \end{minipage}
    \hfill
     \begin{minipage}{0.6\textwidth}
	    \includegraphics[width=7.5cm,keepaspectratio]{images/fx_example_02.png}
    	\caption{Newton's method}
    	\label{NewtonMethodFig}
	\end{minipage}
\end{figure}

To state it more mathematically: find the value $x_0$ where the tangent line intersects with the x-axis. That will be $x_1$.
By iterating this procedure we get an array $(x_k)_{k=0,1,\ldots}$.
From the geometrical aspect of the procedure, we can give a formula between $x_{k+1}$ and $x_k$:
\[
x_{k+1} = x_k - (f(x_k) / f'(x_k))
\]
(that is for finding the zero of $f$)
The idea is that the array $(x_k)$ converges to the value of x where $f(x) = 0$.\\

The goal is not to find the point where $f$ crosses the x-axis, but to find an minimum for $f$. This means finding the point where the derivative of $f$ is zero. This can be accomplished with Newton's method by substituting $f'$ for $f$ and $f''$ for $f'$.
\[
x_{k+1} = x_k - (f'(x_k) / f''(x_k))
\]
And example is depicted in (fig \ref{NewtonMethodFig}).\\

In order to know if $f'(x)$ points to a minimum of f, we need to look at the second derivative $f''(x)$:\\
$f''(x) > 0 \Rightarrow$ f has a minimum at x\\
$f''(x) < 0 \Rightarrow$ f has a maximum at x\\
$f''(x) = 0 \Rightarrow$ inconclusive, perhaps an inflection point\\


\subsection{Functions with two or more variables and their derivatives}

A function with one variable describes the result of a calculation with respect to just 1 variable. In practice the result of a function is dependent on more that one variable. For example the price of a house is not only dependent on the floor area of the house, but for example the distance to the nearest shops count, the number of crimes in the area per year etc. This means the function has multiple variables. This is what we see in machine learning most of the time too. An example of a graph of the multi value function can be seen in figure \ref{3Dsurface}. This depicts the function $f(x,y) = 85 - \frac{1}{90}x^2(x-6)y^2(y-6)$. As you can see the function is defined by two variables: $f(x,y)$.\\
	\\
	Finding the slope in a point using a function with one variable is easily done by determining the derivative. For multi value functions this is done in only 1 direction at a time. This means the derivatie has te determined with regard to 1 variable the rest of the variables is considered as a constant. \cite{calcworkshopcom2022} describes this very well. 	\\
	Mathematically:
	\[
\frac{\partial f}{\partial x} = f_x(x, y) = \lim_{\Delta x \to 0} \frac{f(x + \Delta x, y) - f(x, y)}{\Delta x}
\]

\[
\frac{\partial f}{\partial y} = f_y(x, y) = \lim_{\Delta y \to 0} \frac{f(x, y + \Delta y) - f(x, y)}{\Delta y}
\]

\subsection{Gradient descent with two or more variables}
\label{GradientDescent2}
Considering the metaphore in section \ref{GradientDescent1} the x is the number of steps to walk to the west and y is the number of meters to walk to the north, instead of just walking straight ahead.

With a function $f(x, y)$ of more variables, we can determine the gradient:
\[
\nabla f(x,y) = \left( \frac{\partial f}{\partial x}, \frac{\partial f}{\partial y} \right)
\]
The method is the same, but where we took the derivative for one variable, we will now take the gradient, and the recursive definition of our array  $(x_k,y_k)$ becomes:\\
\[(x_{k+1},y_{k+1}) = (x_k,y_k) - \nabla f(x,y) \cdot \alpha\]



\subsection{Hessian matrix}
\label{HessianMatricesSub}
The second derivative of a function tells something about the curvature of a function in a certain point. A function is convex at a point when the second derivative is positive, and concave if it is negative. When the value of the second derivative is zero the point is an inflection point, a point at which te curvature changes sign. This is possibly a minimum or maximum of the function.

A Hessian matrix is a squared matrix containing all second derivatives of a multi valued function.
Say we have function $f(x,y)$ of 2 variables. Then we get a square matrix having te following format:
\[
H_f(x,y) = \left[
\begin{matrix}
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\[1em]
\frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
\end{matrix}\right]
\]

The Hessian matrix is a symmetric matrix, because of Clairautâ€™s Theorem. This theorem states that $\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}$.

With Hessian matrix local extremes of a function can be found. It helps to identify saddle points, local minima and local maxima. A saddle point (figure \ref{SaddlePoint}) is a function that does neither contain a local maximum or local minimum. From this point a function in some directions (using a certain set of variables) is a maximum and in some points a minimum. The type of extreme can be found by calculating the eigenvalues of the Hessian matrix (section \ref{Newton2}).

The lineair algebra part of Hessian matrices is not in the scope of this report. If you want to to have a thourough explanation, \cite{nocedal2006numerical} is a good reference. 
Hessian matrices can be become quite large. The matrix for a function with $n$ different variables the matrix will have the size of $n \times n$ (in modern machine learning models the value of $n$ can be several billions). The size of the matrix to be calculated and stored in memory will have a size of order $n^2$. For such situations approximations of the Hessian Matrix are being used. A often used algorithm that uses an approximation is BFGS.

\begin{figure}[htbp]
     \begin{minipage}{0.4\textwidth}
	    \includegraphics[width=7.5cm,keepaspectratio]{images/zadelpunt.png}
    	\caption{A saddle point}
    	\label{SaddlePoint}
	\end{minipage}
    \hfill
    \begin{minipage}{0.6\textwidth}
	    \includegraphics[width=7.5cm,keepaspectratio]{images/HessianMatrixGrafiek.png}
    \caption{$f(x, y) = x^4 + y^4 - 4xy + 0.5x^2 + 0.5y^2$}
    \label{HessiaansVoorbeeld}
    \end{minipage}
\end{figure}
\\
BEGIN VOORBEELD AANVULLEN\\
\\
How does this work in practice? This is a concrete example for the function $f(x, y) = x^4 + y^4 - 4xy + 0.5x^2 + 0.5y^2$ (fig \ref{HessiaansVoorbeeld}).

The first order derivates are:\\

\begin{tabular}{l{3,5cm}l{3cm}}
\( \frac{\partial f}{\partial x} = 4x^3 - 4y + x \) & \( \frac{\partial f}{\partial y} = 4y^3 - 4x + y \) \\
\end{tabular}
\\
\\
The second order derivatives are:
\\
\\
\begin{tabular}{l{3,5cm}l{3cm}}
\( \frac{\partial^2 f}{\partial x^2} = 12x^2 + 1 \) &\( \frac{\partial^2 f}{\partial x \partial y} = -4 \) \\
\( \frac{\partial^2 f}{\partial y \partial x} = -4\) &\( \frac{\partial^2 f}{\partial y^2} = 12y^2 + 1 \)
\end{tabular}
\\
\\

This creates the following Hessian Matrix:
\[
H = \begin{bmatrix}
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\
\frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
\end{bmatrix}
= \begin{bmatrix}
12x^2 + 1  & 4 \\
4 & 12y^2 + 1 
\end{bmatrix}
\]

For the point x = 1, y = 2 the matrix becomes 
\[
\begin{bmatrix}
12 & 4 \\
4 & 48
\end{bmatrix} 
\Rightarrow \lambda_1 = 30 - 2 \sqrt{85}, \lambda_1 = 30 + 2 \sqrt{85}.
\]
This is inconclusive, so no maximum, minimum or saddle point. \\
For the point x = $\frac{\sqrt{3}}{2}$ and y = $\frac{\sqrt{3}}{2}$ the matrix becomes:
\[
\begin{bmatrix}
10 & 4 \\
4 & 10
\end{bmatrix} 
\Rightarrow \lambda_1 = 30 - 2 \sqrt{85}, \lambda_1 = 9 -\srt{13}.
\]
This is inconclusive. For the point  ddd
\\
EIND VOORBEELD AANVULLEN\\
\\

\subsection{Newton's method with two or more variables}
\label{Newton2}

The main principles of Newton's method with one variable apply to Newton's method with two or more variables.
Say we have function $f(x,y)$ (containing 2 variables).\\

Then here we have it's Hessian matrix:
\[
H_f(x,y) =
\begin{pmatrix}
\frac{\partial^2 f}{\partial x^2} & \frac{\partial^2 f}{\partial x \partial y} \\[1em]
\frac{\partial^2 f}{\partial y \partial x} & \frac{\partial^2 f}{\partial y^2}
\end{pmatrix}.
\]
Now at a given point (x,y) we'll calculate it's eigenvalues $\lambda_1, \lambda_2, \ldots$\\
(A 2 by 2 matrix would have at most two eigenvalues)\\

If the gradient has value (0,0) at point (x,y) then:
\begin{itemize}
    \item If all the eigenvalues of $H_f$ at (x,y) are positive, it's a minimum
    \item If all the eigenvalues of $H_f$ at (x,y) are negative, it's a maximum
    \item If one of the eigenvalues of $H_f$ at (x,y) are zero, it's a saddle point
    \item In other cases, it's inconclusive
\end{itemize}

In Newton's method generalized to more than one variables, the formula for the next point is:\\
\[
(x_{k+1},y_{k+1}) = (x_k,y_k) - ( H_f^{-1}(x_k,y_k) \cdot \nabla f(x_k,y_k) )
\]

\subsection{Example of a function of two variables}
\label{Example2}
We will look at this function (fig \ref{3Dsurface}):
\[
f(x,y) = 85 - \frac{1}{90}x^2(x-6)y^2(y-6)
\]


\begin{figure}[htbp]
    \begin{minipage}{0.6\textwidth}
	    \includegraphics[width=7.5cm,keepaspectratio]{images/3D_plot.png}
    \caption{3D surface of $f(x,y)$}
    \label{3Dsurface}
    \end{minipage}
    \hfill
\end{figure}


Visually we see a possible minimum near point $(x,y) = (4,4)$.
Volgens \cite{calcworkshopcom2022} is de relativiteitstheorie revolutionair.


\end{itemize}

\section{Numerical Examples}

Text\ldots

\section{Collaboration}

Text\ldots

\section{Reflection}

\subsection{Student a}

Text\ldots

\subsection{Student b}

Text\ldots

\bibliographystyle{plainnat} % Kies een stijl, bijvoorbeeld plainnat, abbrvnat, of apalike
\bibliography{references} % Zonder de .bib-extensie

\appendix
\section{Metaphores}
\subsection{Gradient descent}
\label{metaGradientDescent}
The basic intuition behind gradient descent can be illustrated by a hypothetical scenario. People are stuck in the mountains and are trying to get down (i.e., trying to find the global minimum). There is heavy fog such that visibility is extremely low. Therefore, the path down the mountain is not visible, so they must use local information to find the minimum. They can use the method of gradient descent, which involves looking at the steepness of the hill at their current position, then proceeding in the direction with the steepest descent (i.e., downhill). If they were trying to find the top of the mountain (i.e., the maximum), then they would proceed in the direction of steepest ascent (i.e., uphill). Using this method, they would eventually find their way down the mountain or possibly get stuck in some hole (i.e., local minimum or saddle point), like a mountain lake. However, assume also that the steepness of the hill is not immediately obvious with simple observation, but rather it requires a sophisticated instrument to measure, which the people happen to have at that moment. It takes quite some time to measure the steepness of the hill with the instrument. Thus, they should minimize their use of the instrument if they want to get down the mountain before sunset. The difficulty then is choosing the frequency at which they should measure the steepness of the hill so as not to go off track.\\
\\
In this analogy, the people represent the algorithm, and the path taken down the mountain represents the sequence of parameter settings that the algorithm will explore. The steepness of the hill represents the slope of the function at that point. The instrument used to measure steepness is differentiation. The direction they choose to travel in aligns with the gradient of the function at that point. The amount of time they travel before taking another measurement is the step size. \\

	Copied from \cite{Wikipedia2025_gd}.
\end{document}  