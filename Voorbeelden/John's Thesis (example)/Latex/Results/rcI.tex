\pagebreak
\chapter{Datasets used}
\label{secRCI_II}


Phase 2 of the CRISP-DM method concerns the understanding of the data. Understanding the data means a dataset must be found or created corresponding to the research aim. 

Phase 3 of the CRISP-DM method represents the data preparation during the modeling phase; it is convenient to be independent of the data's original format. Therefore, the datasets are converted to a common format to simplify the data processing. \\


This chapter describes the understanding of the data and the data preparation of the CRISP-DM method because they are tightly linked and implemented in the "Corpus" project, as depicted in figure \ref{imgtotalarch}. Research questions RQ1: "Which are known representative corpora for training and validation?" and RQ2: "What metrics can be used to assess the quality of the semantic links?" are answered in this chapter. 
\section{Datasets used in research}
\label{secDatasetsResearch}
Although only a little existing research was performed, the datasets used for existing research were a good starting point for finding datasets. An extended literature review of the existing research into long-form document comparison was conducted to determine whether the datasets used suit the research for this thesis.\\

Every dataset mentioned in table \ref{tabDatasets} has been evaluated in terms of its suitability for this thesis. The selection criteria for finding suitable datasets were that it was available in English and at least one other language (preferably Dutch), and the text had to be separated into sections.\\

Not all datasets match these criteria. The Semantic Scholar Open Corpus (OC) \citep{Bhagavatula2018}, the CL Anthology Network Corpus (AAN) \citep{radev}, and the Semantic Scholar Open Research Corpus (S2ORC) \citep{lo2019} corpora contain scientific documents and are therefore only available in English. The document links are created by using scientific citations within the papers. Because the corpus license is limited, the OC does not contain the whole text but only the title, abstract, and other small parts. This thesis needs full documents to analyze the structure, which makes the OC dataset unsuitable. The documents in AAN are not split into sections. The Chinese News Same Event dataset (CNSE) and the Chinese News Same Story dataset (CNSS) released in \citet{Liu2019} are both in the Chinese language and, therefore, not suitable for this thesis.\\

GWikiMatch, WiRe, and Wikisim are based on a selection of Wikipedia articles. The advantage of using Wikipedia articles is that they can easily be split into sections because this structure exists within the articles. Another advantage of using Wikipedia articles is that Wikipedia is multilingual. A selection of articles is available in both English and Dutch. Lastly, Wikipedia articles donâ€™t have license restrictions preventing their dataset use. For these reasons, these datasets are used for the research. \\

Table \ref{tabDatasets} gives an overview of datasets used or created for the document-to-document comparison in recent papers. Using the considerations previously mentioned, the following datasets were chosen to be used in this thesis:

\begin{itemize}
  \item \textit{S2ORC}, a dataset containing scientific papers written in the English language
  \item \textit{GWikiMatch - EN}, manually curated and automatically created links between a subset of Wikipedia articles in the English language
  \item \textit{GWikiMatch - NL}, the same articles as \textit{GWikiMatch - EN} but then in the Dutch language
  \item \textit{WiRe - EN}, a small dataset with manually curated links between a subset of Wikipedia articles in the English language
  \item \textit{WiRe - NL}, the same articles as \textit{WiRe - EN} but then in the Dutch language
  \item \textit{Wikisim - EN}, an even smaller dataset with manually curated links between a subset of Wikipedia articles in the English language  
  \item \textit{Wikisim - NL}, the same articles as \textit{Wikisim - EN} but then in the Dutch language
\end{itemize}

\\

\begin{table}[h!tbp]
    \centering
    \captionsetup{justification=centering}
\scalebox{0.78}{
    \begin{tabular}{l|l|l|l|l|l|l|l|l}
    	\textbf{Paper} &  \textbf{OC} & \textbf{AAN}  
    	& \textbf{S2ORC}  & \textbf{CNSE/CNSS} & \textbf{Wikipedia} & \textbf{GWikiMatch} & \textbf{WiRe} & \textbf{Wikisim} \\
		\hline
        \citep{Witten2008}& &  & &  & &  & created &  \\ 
		\hline
        \citep{Ponza2017} & &  &  & &   & & & created   \\ 
		\hline
        \citep{jiang2019semantic} & & used & & & used & & &  \\
		\hline
        \citep{512Tokens} & & used & &  & used & created  & &  \\
		\hline
        \citep{zhou2020multilevel} & used & used & used &  &  &  &  \\ 
		\hline
        \citep{Pang2021}& used & used & used & used &  & & \\ 
		\hline
    \end{tabular}}
    \caption{Datasets used by the research on long-document to long-document comparison.}
  	\label{tabDatasets}
\end{table}
Each of the datasets will be described in the following paragraphs.

\subsection{The S2ORC dataset}
\label{secS2orc}
For this thesis, version 2020-07-05 of the S2ORC dataset is used. It contains the metadata of about 136 million papers and the full text of 81 million papers. The full text has been parsed from PDF and XML into a generic JSON format that has the text divided into sections, and the citations have been resolved to usable paper IDs. That means we can use this set or create links between documents and between sections and documents.\\

The S2ORC dataset has to be preprocessed to be used for this thesis. First, the citation texts are removed from the section text to prevent the algorithm from selecting citation texts for creating links. Second, because this thesis aims to create an algorithm that uses a small set of documents, a small portion of the S2ORC dataset is selected. The field of study groups the papers in S2ORC. This thesis selects a small field of study with a limited amount of papers: about 10,000 documents with history as a subject.\\

Although the software was created to process the S2ORC files, the S2ORC dataset was not used for the research. First, the dataset was completely in English and could not be easily translated. Second, the documents' structure and length differ from the Wikipedia articles, and because of time limitations, the focus of the research was moved to Wikipedia articles.


\subsection{GWikiMatch}
\label{sectWikiMatch}
As part of their work, \citet{512Tokens} released a benchmark that researchers can use for comparing different document-matching methods. The dataset is called GWikiMatch: "a benchmark dataset for the long-form document matching task, which is to estimate the content semantic similarity between document pairs." The dataset is based on English articles from Wikipedia. Although the dataset was generated in 2021, it is used in this thesis because changes in the text articles probably do not affect the relationship between articles.\\

The dataset was generated in four steps. First, an article graph was constructed from Wikipedia, on which several algorithms were applied to generate article pairs. Second, each document pair was annotated by three human raters on a crowdsourcing platform. The Majority vote rating was used as the content similarity label, which was either 0 (not similar), 1 (somewhat similar), or 2 (strongly similar). A total of 11K document pairs were annotated. Third, by using negative sampling, the dataset was enlarged and made more balanced. After finishing the third step, the assigned labels to the document pairs were either 0 (not similar) or 1 (somewhat similar). Last, controversial topics were removed by further human annotation. The dataset and details of the generation of the dataset can be found on GitHub\footnote{https://github.com/google-research/google-research/tree/master/gwikimatch}. Unfortunately, the document pairs humans annotated in the second step are not publicly available. These document pairs could have been used to identify links that were strongly similar and not similar, leaving out the somewhat similar links. That would have made the dataset more unambiguous.  \\

For this thesis, a script was created that translates the names of the articles to Wikidata labels. This translation is done for two reasons. First, the software developed for extracting articles from Wikipedia dumps can be reused using these labels. Second, using the Wikidata labels, the corresponding articles in Dutch can be retrieved. Thus, a Dutch version of the GWikiMatch dataset can be created. The script assumes that when article $A$ is related to article $B$, this relationship can be reversed so that article $B$ is also related to article $A$. Because not every English Wikipedia article is available in the Dutch Wikipedia, the values for Dutch are significantly lower. Nevertheless, the number of articles for both languages is sufficient for the research.

\subsection{WiRe \& Wikisim}
\label{sectWiReWikisim}
To validate their research \citet{Ponza2017} have created a dataset and made it publicly available for other researchers to validate their work. The dataset is called WiRe\footnote{https://github.com/mponza/WikipediaRelatedness/tree/clean/src/main/resources/datasets/WiRe.csv}. The dataset contains 503 links from the English Wikipedia that have been assigned a score between 0 and 10. The construction of the dataset was done in three steps. First, the dataset of 100,000 entity pairs constructed from New York Times articles \citep{Dunietz2014} was used as a base. Second, the resulting dataset was filtered and balanced by several algorithms. Last, two human evaluators assigned every article pair of the resulting dataset a score between 0 and 10. When their scores did not coincide, a third human evaluator discussed the score with the original evaluators. The process resulted in the WiRe dataset. A thorough process description can be found in \citep{Ponza2017}.\\

Another dataset that \citet{Ponza2017} has used for validation is the Wikisim dataset\footnote{https://github.com/mponza/WikipediaRelatedness/tree/clean/src/main/resources/datasets/Wikisim.csv}. It was created for validating the semantic relatedness obtained from Wikipedia links as described in \citep{Witten2008}, based on the WordSim 353 dataset (\citep{word353}), humans annotated it. The document pairs have been rated with a value between 0 and 1. Unfortunately, the details of the annotation process are not described in the paper.

Analogous to the GWikiMatch dataset, software that translates the Wikipedia IDs to Wikidata labels was developed. This software enables the translation of the articles to Dutch and reuses already-created software. The relationships are, as in the GWikiMatch dataset, also reversed. The Wikisim dataset's score, which has a value between 0 and 1, is translated to a value between 1 and 10, so it is compatible with the WiRe dataset. The resulting datasets and the cumulative number of documents in the dataset are depicted in figure \ref{imgWiReDistr} and listed in table \ref{tabCutoff}. As can be seen from this table, the cutoff value is 6. Using this cutoff means that every value between 0 and 6 is translated to a resulting value of 0 and 1 otherwise. The resulting values are almost evenly distributed, which is what one would choose intuitively.


\section{Metrics}
\label{secMetrics}
As stated before, choosing the right evaluation metrics is vital for validating models and comparing the newly created software to other research. Section \ref{F1Score} and \ref{ranking} describe calculating the F1-score as a basic F1-score and the F1-score based on ranking, respectively. After doing literature research, it was found that existing research into creating document links mostly uses the basic F1-score as a validation metric. The chosen datasets only contain absolute values (0 for a non-match and 1 for a match). Unfortunately, no ranking is available. That makes it impossible to make use of ranking scores.
\afterpage{
	\begin{figure}[h]
	\centering
\captionsetup{justification=centering}

\includegraphics[scale=0.8]{images/WiReWikisimDistribution.pdf}
	
\caption{The distribution of the WiRe and Wikisim datasets where the validity of the relation has a value between 0 and 10.}
	
\label{imgWiReDistr}
	
\end{figure}
	

	\begin{table}
    \centering
    \captionsetup{justification=centering}
	\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l}
	        & 0  & 1  & 2  & 3   & 4   & 5   & 6   & 7   & 8   & 9   & 10   \\
	\hline
	\textbf{WiRe}    & 26 & 63 & 94 & 110 & 141 & 178 & \textit{\textbf{246}} & 346 & 478 & 503 & 503  \\
	\hline
	\textbf{Wikisim} & 2  & 8  & 24 & 45  & 64  & 90  & \textit{\textbf{137}} & 192 & 255 & 267 & 268 \\
	\hline
	\end{tabular}
	    \caption{Cumulative number of pairs per dataset. The cutoff is shown in italics}.
	\label{tabCutoff}
	\end{table}
}
\pagebreak


\section{Data preparation}
\label{secDataPreparation}

For every dataset, software was developed to perform the needed preprocessing and translate the data to a common file format. For several reasons, XML was chosen as the common file format. Firstly, it can easily be created and read because of the availability of software libraries for almost every computer language, including Python. Secondly, it is easily readable by humans. Lastly, the XML format does not restrict the structure to be used. \\
The scripts are publicly available on GitHub\footnote{https://github.com/johnstegink/Wikidata-corpus} % todo: link nog aanpassen, dit is vast nog niet goed
. The code is explained with comments inside the code. The accompanying readme file describes the parameters of the scripts for generating the datasets. For convenience, this readme file is included in Appendix B.\\

The corpora use Wikipedia as the text source. The Wikipedia texts must be cleaned before they are included in the corpus. The text is cleaned while generating the XML and is not implemented as a separate step. It is started by removing text components that are rare to texts of knowledge bases. Articles containing less than 128 characters are removed because it was found that these articles are either redirecting to another article or need more information. Tables in the text have a lot of information, albeit in a structured manner that is helpful for human readers. This structure, however, is complex to read for NLP algorithms and is therefore removed. This is because the focus of this thesis is on sections and documents. The same applies to enumerations. The article \textit{"Merrick (surname)"}, for example, contains a list of (famous) people having the surname "Merrick." By making random spot checks, it was found that most enumerations in Wikipedia do not contain complete sentences but consist of a few terms. For that reason, enumerations are removed too.\\

Specifically, Wikipedia articles contain explicit metadata that is not generally contained in texts. This surplus of information 
must be removed to avoid the algorithm using this information to create semantic links. That would make the algorithm unsuitable for use in plain text. For this reason, hyperlinks are simplified so that the target is removed and only the link's text is maintained. Finally, sections containing references and categories are removed from the text entirely.\\

