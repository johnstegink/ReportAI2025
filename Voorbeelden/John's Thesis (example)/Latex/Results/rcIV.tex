\pagebreak
\chapter{Implementing the base algorithm}
\label{secRCIII}
The CRISP-DM method models an iterative process. For this thesis, the iterations occur during phases 4, the modeling phase, and 3, the data preparation phase. The first iteration focuses on implementing the base algorithm and validating the results. Subsequential iterations attempt to improve on the base algorithm.\\

The first phase of the LHA algorithm, the matching of documents, as described by \citet{nikolov2018large}, is implemented as depicted in figure \ref{imgLHAphasesOriginal}. The results of SADDLE are compared with those from the original LHA algorithm. The second phase of the LHA algorithm finds matching sentence pairs in the matched documents. However, this thesis aims to find similar sections in the documents. Hence, the results from the second phase cannot be used for validation. 

\section{General software principles}
\label{secGeneralPrinciples}

The software for the three phases of SADDLE is written in Python. The scripts that string the Python programs together are written as scripts for the Unix shell called Z shell (Zsh)\footnote{https://www.zsh.org}. Zsh is an extended Bourne shell with many improvements, including some features of Bash, ksh, and other interpreters. Although SADDLE was developed on macOS, Python and Zsh enable the software to run on a Linux system. Many examples and modules are available for Python, saving time developing and testing the software. Because Python does not do compile time type checking, the number of errors while running the programs (and therefore development time) is more significant than when using type-checked languages like C\# or Java. The Python code is divided into classes as much as possible to improve the isolation of functionality and make the code easier to read.\\

The SADDLE software is written to support this research project, having some basic principles as a point of departure:
\begin{itemize}
  \item The code is easy to change because the needed functionality is unknown beforehand.
  \item The software is split into independent programs because parts must be run multiple times for the desired results.
  \item Contrary to a production system, high performance is not required. However, because the parts of the software will be run multiple times, caching is used as much as possible. The data structures are written to and read from disk using the Python Pickl\footnote{https://docs.Python.org/3/library/pickle.html} module for this purpose.
\end{itemize}

\section{The first phase}
\label{secIteration1}
By duplicating the results of the original LHA algorithm the implementation of the first SADDLE phase can be validated.
 The LHA algorithm works in two phases to generate sentence alignments (figure \ref{imgLHAphasesOriginal}). The LHA algorithm's first phase implements document alignment using embeddings and word similarity methods. From \citep{nikolov2018large}, it can be concluded that using embeddings gives better results than word similarity. Therefore, this thesis focuses on using embeddings (table \ref{tabLHAPerformance}). The source code of the LHA algorithm is publicly available\footnote{https://github.com/ninikolov/lha}, so this can be used as a reference for implementing the algorithm for this thesis.\\


\begin{table}[h!tbp]
    \centering
    \captionsetup{justification=centering}
    \begin{tabular}{l|l|l|l|l}
    	& \textbf{Approach} & \textbf{EDim} & \textbf{F1}\textsubscript{max} & \textbf{TP} \\
		\hline
		\parbox[t]{2mm}{\multirow{2}{*}{\rotatebox[origin=c]{90}{\textit{emb.}}}} 
		& Average word embeddings (Avg) & 300 & 0.66 & 43\% \\		
		& Sent2Vec \citet{pagliardini2017unsupervised} & 600 & 0.78 & 61\% \\	
		\hline
    \end{tabular}
    \caption{Automatic evaluation of Document alignment. EDim is the embedding dimensionality. TP is the percentage of true positives obtained at F1\textsubscript{max}. The information in this table is copied from \citet{nikolov2018large}}.
  	\label{tabLHAPerformance}
\end{table}

Parts of the LHA algorithm are used for SADDLE. This way, the output of the two algorithms can be compared for validation, and the wheel does not have to be re-invented. However, it is implemented so that future improvements can easily be made. As shown in the architectural schema, figure \ref{imgLHA_v1}, the process consists of three steps.

\begin{itemize}
  \item {\textbf{Read documents}}. The documents are read from the corpus in the common XML format (section \ref{secDataPreparation}). The text from every section is concatenated into one large text. 
  \item {\textbf{Clean text}}. All unknown character entities are removed, digits are replaced with a symbol, and multiple white spaces are reduced to one. Next to these basic text replacements, stopwords are removed (language-sensitive), and the text is converted to lowercase.
\item{\textbf{Create embedding}}. The embedding is created using the Sent2Vec algorithm or using word embeddings. The original LHA algorithm uses pre-trained versions of these models. Because Sent2Vec is pre-trained in English only, this can only be used for the  English corpora. A pre-trained Word2Vec model (GloVe \citep{glove}) creates embeddings for the Dutch corpora. In future iterations, other, probably better, algorithms for embedding are used.
\end{itemize}

Using the embeddings of step 1 and step 2, semantic links between documents can be constructed. The original LHA algorithm has divided this step into two substeps for performance reasons. First, it filters the most relevant documents using an approximate nearest neighbor algorithm. The second step uses these approximate nearest neighbors to calculate the exact nearest neighbors using cosine similarity. This step uses the ANNOY algorithm to search for the nearest neighbors efficiently. SADDLE calculates the cosine similarity for all document pairs, thus skipping the first step. As a result, the approximate nearest neighbor algorithm does not have to be used. Hence, the quality of the approximation does not have to be evaluated.\\

The similarity values of the document pairs found using the algorithm described in the previous paragraph are used to determine the ranking of the document pairs. The algorithm uses every document's $K$ nearest neighbors as a valid document pair. The last step calculates the F1 and the true-positive percentage score based on the filtered pairs.\\

Implementing the first phase of SADDLE must be validated against the original algorithm to get a correct baseline. The results of the original algorithm are quantified as an F1 score and true positive percentage. \cite{nikolov2018large} do not describe the method used for calculating the F1 score, nor does the source code contain an implementation of this calculation. Because it is unknown whether the F1 score is based on ranking or another method is used, the F1 score used in the paper cannot be compared. That means that the calculation of the true positive percentage must be used for validating the implementation. The following paragraphs describe the steps taken to validate SADDLE.\\

First, the source code of the original code was examined, and the same methods and libraries were used for the new implementation. Next, the hyperparameters and models that were hard-coded were copied. For example, all documents were embedded at once with Sent2Vec\footnote{https://github.com/epfml/Sent2Vec} using the pre-trained model "Wiki Unigrams." Next, the average word embedding was calculated using the model "GoogleNews vectors negative300" using the Python module Gensim\footnote{https://radimrehurek.com/gensim}.\\

The same input has to be used to compare the new implementation results with the results shown in table \ref{tabLHAPerformance}. For these results, \citet{nikolov2018large} used the 46 wiki article pairs from Wikipedia and Simple Wikipedia \citep{hwang2015} where 1000 randomly sampled articles from Wikipedia and Simple Wikipedia were added ($1046 \times 1046$ article pairs in total). \citet{hwang2015}, selected 46 article pairs that began with the letter 'a' at random from Wikipedia.\\

The first test of the new implementation was conducted by writing a tool that selected 46 random articles from the latest Wikipedia and Simple Wikipedia, starting with the letter 'a'. Additionally, 1000 random articles were added. Unfortunately, the quality of the results was poor: the number of true positives found was much lower than those shown in table \ref{tabLHAPerformance}, even after the code was closely compared to the original and appropriate adjustments were made. Furthermore, the average length of Wikipedia articles has grown in the last decade ("proportionally more content is added to existing articles rather than new articles ... being written"\footnote{https://en.wikipedia.org/wiki/Wikipedia:Size\_of\_Wikipedia}). These lengthy articles are potentially more complex to match Simple Wikipedia articles. Because the original set was generated from the Wikipedia dumps of June 2012, and the tool uses the dumps of August 2022, the increase in length can explain the difference in results. Another explanation is that the original set of 46 articles was not entirely random. To overcome this problem, the authors of \citep{hwang2015} were contacted. Unfortunately, this was in vain because the original set was no longer available online. Luckily, by reverse engineering an old copy of the dataset, the titles of the original articles were recovered. \\

The test was conducted again by extracting the articles with these titles from an older Wikipedia dump. Because the dumps of 2012 and 2013 were not available online anymore, and the format of the dump of 2011 and earlier was challenging to work with, the first available dump of 2014 was chosen (Juli 2014 for  Simple Wikipedia and August 2014 for English Wikipedia). The following articles are missing in this dump: "Academy Award for Best Picture (1950s)", "Academy Award for Best Picture (1930s)", and "American Basketball at the Olympics."\\

The true-positive percentage of 61\% for Sent2Vect was converted to an absolute number of true positives: 28. The number of true positives found with the new iteration is 29. The slight difference can be explained by the fact that Wikipedia articles from 2014 were used instead of those from 2012. During that period, the content of the articles has probably changed. That means the first phase of the LHA algorithm was reimplemented correctly.\\

Another, not exact way to validate the results is to check random document pairs to see whether they seem reasonable. Checking random pairs does not validate the algorithm extensively, but it is an easy way to spot errors. Figure \ref{imgRelationsHtml} shows a part of an interactive HTML page created by the script that creates the document relations. The example shows English Wikipedia articles to which the script has added a relation. The similarity of the articles is shown in the third column. The Wikipedia articles are identified by the title of the article and the Wikidata ID. By clicking on a Wikipedia article identification, the corresponding article in Wikipedia is opened in a separate browser window. 

 

 

\begin{figure}[h]
\centering
\captionsetup{justification=centering}
\includegraphics[width=70mm]{images/Example_html_doc_relation}
\caption{Part of an HTML file that shows the document relations. The similarity of the Wikipedia article pair is shown in the third column.}
\label{imgRelationsHtml}
\end{figure}

% deze kunnen worden gemaakt met: 
%     /Users/jstegink/Dropbox/John/Studie/OU/Afstuderen/Thesis/Code/Model/visualize.py 
%          -m /Users/jstegink/thesis/output/metrics 
%          -o /Users/jstegink/Dropbox/John/Studie/OU/Afstuderen/Thesis/Document/Latex
%.        -k 5   

\section{The second phase}
\label{phase2}
The second phase of SADDLE aligns the document pairs sections found using the first phase (see \ref{secSADDLE}). The script created as described in this section was extended with functionality to calculate the section embeddings. The embedding of the sections is automatically added to the generic XML output so it can be used for phase 2. Extending the script created in \ref{secSADDLE} instead of creating a new script has multiple advantages. Firstly, because the same code base is used, the validation of the code has already been done. Secondly, the maintenance of the embedding calculation is optimized because there is only one implementation. Thirdly, the scripts run more efficiently because the files only have to be analyzed once. The disadvantage is that the document pairs will also be calculated when the section embeddings have to be calculated for one reason or another. Another disadvantage is that the identical algorithm is used for document and section embeddings. The algorithm for calculating the optimal embedding of a document does not necessarily mean that this algorithm is optimal for the calculation of section embeddings. Further research can differentiate between these algorithms.\\

The section embeddings calculated in the first phase are passed to the second phase to create relations between sections. The relations are created only for the document pairs determined in the first phase. 
The section relations are created using an algorithm as described in section \ref{secSADDLE}. First, the nearest neighbors are calculated by creating a similarity matrix. A similarity matrix is a two-dimensional matrix where the sections are enumerated in rows and columns. Each cell in the matrix contains the cosine similarity of the two sections. A SciPy (open-source software for mathematics\footnote{https://docs.scipy.org}) function is used to create this matrix. Thus, the creation of the matrix is validated, and the development time of the second phase is reduced. The elements of this matrix that contain the distance between sections themselves (having a value of 1.0 by definition) and the elements that have a similarity below the threshold are removed. Lastly, after sorting the elements, only the top K elements are used to create the section relations.\\

The validation of phase 2 consists of creating HTML files to enable visual inspection. This validation is not exact, but it helps to get an intuition of the output of phase 2, thus spotting errors in implementing the software for this phase. The actual validation can be done after phase 3 is finished.\\

Creating the HTML files is crucial because this output can be used as a base for the final user interface of the SADDLE algorithm (see section \ref{secSADDLEVerification}). The output consists of a directory with HTML files that contain all functionality to prevent the need to use a web server. Using a web server would complicate using SADDLE because a web server must be installed and configured. The layout of the HTML files is simple but usable. The functionality is implemented using the following open-source libraries: 
\begin{itemize}
  \item Bootstrap\footnote{https://getbootstrap.com} to have a head start implementing the layout
  \item JQuery\footnote{https://jquery.com/} to simplify using JavaScript in HTML pages
  \item LeaderLine\footnote{https://anseki.github.io/leader-line} to help create the dynamic arrow between the sections.
\end{itemize}

\section{The third phase}
\label{phase3}

The input for phase 2 (the document and section embeddings) will also be used for phase 3. Whereas phase 2 focuses on the similarity of sections within a document, phase 3 focuses on the similarity of two documents. It uses the section embeddings to determine this similarity. A neural network learns the relationship between section embeddings and document similarity. This neural network will help to answer the research question of section \ref{secResearch}.\\

The datasets mentioned in chapter \ref{secRCI_II}: gWikiMatch, Wikisim, and WiRe are used for the training phase of the neural network. These datasets contain human-validated Wikipedia article similarities. Unfortunately, the Wikidata datasets cannot be used for training the neural network because of the lack of validated similarities. Because the datasets consist of Wikipedia articles, the implicit assumption is made that Wikipedia articles are a good representation of documents in general. The validity of this assumption was not researched. Because of the relatively small number of documents in these datasets, the neural network cannot become too complicated (have too many nodes). As a rule of thumb, the number of training examples needed to train a neural network increases when the number of nodes in the network increases\footnote{More precisely, the amount of training data required is determined by the number of trainable parameters among other factors such as the use of regularization. However, for most neural network structures, including the ones used in this thesis, the number of trainable parameters is proportional to the number of nodes. Therefore, this statement is valid.}.\\

Existing neural networks were considered suitable for finding a suitable neural network. As mentioned before, most existing neural networks for document similarity are complicated and, therefore, unsuitable for small datasets. SMASH RNN \citep{jiang2019semantic} is a potential candidate to base the neural network of phase 3 on. A considerable amount of time was spent during the research for this thesis to implement this network. Unfortunately, the paper does not contain the source code for the algorithm. An implementation found on the internet still needed to be finished and contained errors. Despite the time spent implementing the SMASHH RNN network, it was chosen to discontinue using it.\\

A simple neural network, written in PyTorch\footnote{https://pytorch.org/}, was implemented for this thesis. PyTorch is one of the most popular Python-based frameworks for implementing neural networks.\\

Tuning a neural network to get optimal results takes a lot of time. Because of the limited time for this thesis, it was chosen to spend only a little bit of time on tuning and constructing this network because the aim is to research whether section similarity can be used to find document similarity. Tuning this neural network optimally is left for further research. The chosen network (described in chapter \ref{secResults}) consists of one hidden layer and a single output node representing the chance that two documents in a document pair are similar. Thus, the number of nodes in this network is kept down. The datasets contain humanly annotated document pairs, which means the neural network is trained using supervised learning. \\

%todo: volgende paragraaf nog aanpassen
As shown in (table \ref{tabWholeDocument}), the number of validated pairs for Wikisim and WiRe is relatively small. Therefore, K-Fold Cross-Validation is used for the training set. This information is used as input for calculating the F1 score(see section \ref{F1Score}), and the quality of the neural network output is determined using the F1 score.\\ 

Second, comparable to the validation of phase 2 of SADDLE, a graphical intuitive validation is done using heatmaps in the third phase. A heatmap is a visual representation of data that uses a system of color coding to represent different values. The values used as input for the neural network are abstract and complex to interpret by humans. Heatmaps help enable quick analysis of this data by visually detecting patterns. A value of 1.0 is represented using black, going down via shades of red and yellow to white, representing the value of 0.0. The x-axis represents the embeddings from the left part of the document pair, and the y-axis represents the embeddings from the right side of the pair. The heatmaps are grouped into directories on the file system containing true positives, false positives, true negatives, and false negatives to simplify the interpretation of the heatmaps (see figure \ref{imgHeatmaps}).\\

\begin{figure}[h]
\centering
\captionsetup{justification=centering}
\includegraphics[width=155mm]{images/heatmaps.png}
\caption{Two example heatmaps. These heatmaps display the matching scores of sections 1–12 of the first document (x-axis) compared against sections 1–12 of the second document (y—axis), with darker colors indicating stronger matches. The left heatmap represents an invalid document pair.}
\label{imgHeatmaps}
\end{figure}


All neural networks use an input vector of a size that is equal for all examples to be trained on or to predict the output. A similarity matrix of all section vectors of the document pairs is calculated to compose the input vector for the neural network. Each cell in the matrix contains the cosine similarity of the sections. The dimensions of this matrix are $M \times N$, where $M$ is the number of sections in the first document of the document pair, and $N$ is the number of sections in the second document of the document pair. The cells in the similarity matrix contain the cosine similarity of the two sections. The values of the similarity matrix are calculated using SciPy. \\

Not all similarity matrices have equal dimensions, i.e., $M$ and $N$ are different per document pair. Because the input vector must be fixed-sized, a method for constructing a fixed-size vector for every possible similarity matrix was developed. $Q$ is a quantity introduced for creating the input vector, where the size of the fixed length vector is represented by $Q^2$. The vector is constructed by concatenating the rows in the similarity matrix, as illustrated in figure \ref{imgMatrixToVector}.


\begin{figure}[h]
\centering
\captionsetup{justification=centering}
\includegraphics[width=155mm]{images/sim_matrix_to_vector.drawio.pdf}
\caption{The similarity matrix of size $N \times M$ mapped to a vector of size $Q^2$.}
\label{imgMatrixToVector}
\end{figure}


Five situations can occur when constructing the input vector. The first situation is when $M < Q$ indicates that too few rows are available in the similarity matrix to create the vector. In this case, the matrix will be padded with $Q - M$ rows, where the values of each row are all zero. The second situation is when $N < Q$ indicates that too few columns are available in the similarity matrix to create the vector. Equivalent to the previous situation, the matrix will be padded with $Q - N$ columns, where the values of each column are equal to zero. The third and fourth situations are when $M > Q$ or $N > Q$, i.e., too many values are available to build the input vector, and the rows or the columns, respectively, are too long. To solve this problem, either an average of the additional values can be used as a value for the last cell of the row or column, or the additional values are not being considered, i.e., the rows or columns are truncated. For this thesis, the last alternative was implemented for reduced complexity. The last situations are when $Q = M$ or $Q = N$; these are ideal because nothing has to happen.\\


The vector can contain 0.0 values when the number of sections in a document is smaller than $Q$ (see the heatmap in figure \ref{imgHeatmaps}). A value of 0.0 does not mean that it should be regarded as an absolute value of zero, but instead, it means that the value is irrelevant to the similarity. A mask can be added to the vector to help the neural network distinguish between relative and irrelative values. This mask has a value of 1.0 for the relevant cells (colored cells in figure \ref{imgHeatmaps}) and 0.0 for irrelevant cells (white cells in figure \ref{imgHeatmaps}). Hence, the length of the vector becomes $Q^4$. Using a mask increases the dimensions of the input vector, and thus, the complexity of the neural network increases. This mask can be used only if enough training examples are available.\\

\begin{figure}[h]
\centering
\captionsetup{justification=centering}
\includegraphics[width=155mm]{images/heatmap_mask.png}
\caption{A heatmap of a document pair consisting of documents with 4 and 8 sections, respectively.}
\label{imgHeatmaps}
\end{figure}


The last problem to be solved is determining the correct value of $Q$. When $Q$ is too large, the network will become too deep, so it needs more data to be trained on. On the other hand, when $Q$ is too small, a lot of information is lost, meaning the neural network cannot optimally predict the similarity of a document pair. A script was developed to indicate what percentage of the sections will be higher than Q ($\frac{Q - M}{Q}) \times 100$ or ($(\frac{Q - N}{Q}) \times 100$, a higher value indicates more information will be lost. Table \ref{tabSectioncutoff} of Appendix F suggests that 12 is a good value for $Q$ because when $Q > 12$, the percentages slowly decrease, as opposed to the values where $Q \leq 12$. Figure \ref{imgSectionsHistogram} shows the distribution of the number of sections in all corpora using $Q = 12$.\\

\begin{figure}[h]	\centering
\captionsetup{justification=centering}

\includegraphics[scale=0.23]{images/sections_histogram.png}	
\caption{The summarized number of sections per document for the corpora GWikiMatch EN, GWikiMatch NL, WiRe EN, WiRe NL, Wikisim EN, Wikisim NL, Wikidata EN, and Wikidata NL.
}
\label{imgSectionsHistogram}\end{figure}


An artificial input is constructed to validate the technical aspects of the neural network. This synthetic data is constructed using random values representing an input vector for a valid and invalid document pair. It will contain more values between 0.7 and 1.0 for valid document pairs than when the document pair is invalid. In that case, the vector will contain more values between 0.0 and 0.3. After testing the network with these values, the F1 score should be close to 1.0, and the loss should have a value close to 0.0.\\

%\section{Validation}
%\label{secRCIIIValidation}
%
%The primary purpose of this research is to answer the question: "How can hierarchical decomposition (into sections) and encoding find automated relations between documents in document sets that contain less than 10K documents?". SADDLE attempts to answer this question. A program is developed to validate whether SADDLE improves the quality of these automated relations compared to processing a document in its entirety.\\
%
%The program uses cosine distance to calculate the similarity of the documents. The embedding algorithm equals the algorithm used to calculate the sections' embeddings (initially Word2Vec and Sent2Vec). This way, a fair comparison can be made. The thus computed similarity is compared to a threshold. When the similarity is smaller than the threshold, it is considered a valid relation and invalid otherwise. Because the embedding algorithms are pre-trained, there is no additional training necessary. Hence, the total dataset can be used as a validation set. The same software for calculating the F1-score and other indicators is used as with SADDLE.\\

\section{Hyperparameters}
\label{secHyperParameters}
Wikipedia defines a hyperparameter as follows: "In machine learning, a hyperparameter is a parameter whose value controls the learning process. By contrast, the values of other parameters (typically node weights) are derived via training." Table \ref{tabHyperParameters} contains a short description of all hyperparameters of SADDLE. The optimal values of the hyperparameters are found by trying many combinations of hyperparameters (chapter \ref{secResults}). Because not all corpora get the best results with the same configuration of hyperparameters, an optimal setting was chosen.\\

\label{refHyperparameters}
\begin{table}[!ht]
    \centering
    \captionsetup{justification=centering}
    \begin{tabular}{l|p{6cm}|l|l}
    \hline
        \textbf{Parameter} & \textbf{Description} & \textbf{Where} & \textbf{Reference} \\ \hline
        Language & For this thesis, the values 'en' (English) or 'nl' (Dutch) can be used for the natural language of the Wikipedia articles & proces\_file.sh  & Chapter \ref{secRCIIIb} \\ \hline
        Corpus & The corpus containing the Wikipedia articles. The value of this parameter is 'Wikisim,' 'WiRe,' or 'gwikimatch.' Later, this parameter can be used for custom corpora. & proces\_file.sh & Chapter \ref{secRCIIIb} \\ \hline
        Method & The embedding algorithm for creating embeddings of the documents and sections. Possible values are 'Word2Vec', 'Sent2Vec', 'SBERT,' or 'use.' & proces\_file.sh  & Section \ref{subsecLHAAlgortithm}  \\ \hline
        NN & The K nearest neighbors, used in the document alignment phase of SADDLE. & proces\_file.sh & Section \ref{subsecLHAAlgortithm}  \\ \hline
        Sim & The manually selected threshold for filtering nearest neighbors. & proces\_file.sh  & Section \ref{subsecLHAAlgortithm}  \\ \hline
        Maxdoc & The maximum of document pairs per document to be created during the document alignment phase. & proces\_file.sh  & Section \ref{subsecLHAAlgortithm} \\ \hline
        Sections & Number of sections to consider for the neural network. & proces\_file.sh & Section \ref{phase3} \\ \hline
        Batch size & The batch size for training the neural network. & trainModel.py.  \\ \hline
        Epochs & The number of epochs in which the neural network is trained. & trainModel.py  \\ \hline
        Folds & The number of folds to use for K-fold Cross Reference Validation & & \\ \hline
        Learning rate & The learning rate that is used for training the neural network. & trainModel.py  \\ \hline
        Threshold & This threshold determines whether a document pair is valid or invalid. The cosine value of the calculated embeddings of the document pairs is compared to this threshold. & useEmbeddings.py &  \\ \hline
    \end{tabular}
    \caption{The hyperparameters used in the scripts and programs.}
	\label{tabHyperParameters}
\end{table}

\section{Other languages}
\label{secRCIIIb}

This section describes the considerations for using SADDLE in another language than English to help answer the last research question, "To what extent can this method be extended to a different language than English (preferably Dutch)?". Much information is available in languages other than English, but much natural language processing research focuses on English as a primary language. \\

As described in chapter \ref{secRCI_II}, the datasets used for this research are all based on articles in Wikipedia. Because Wikipedia is multilingual, the IDs of Wikidata can be used to find the translation in the target language of an article written in English. English has the most Wikipedia articles (see table \ref{tabWikipediaLanguages}), meaning a translation is not always available in the desired language. Only articles available in the target language can be used for the dataset in the target language. When few articles are available, the dataset becomes too small to train SADDLE properly. 

\begin{table}[h!tbp]
    \centering
    \captionsetup{justification=centering}
    \begin{tabular}{l|l}
    	\textbf{Language}&\textbf{Number of articles}\\
		\hline
		English        & 6,724,483 \\
		German         & 2,572,818 \\
		French         & 2,558,636 \\
		Dutch          & 2,133,863 \\
		Norwegian      & 616,793   \\
		Irish          & 59,096    \\
		Dutch Low Saxon & 7,778  \\
		\hline
    \end{tabular}
    \caption{A random selection of languages and their number of Wikipedia articles as of October 7th, 2023: https://meta.wikimedia.org/wiki/List\_of\_Wikipedias\_by\_language\_group}
  	\label{tabWikipediaLanguages}
\end{table}


Wikipedia in the Dutch language has approximately one-third of the articles compared to the English language Wikipedia. This means the Dutch Wikipedia is presumably large enough to provide sufficiently large datasets. Unfortunately, Sent2Vec has only been pre-trained in English. Therefore, it cannot be used for other languages, but fortunately, Glove and more recent embedding algorithms have. The results of using SADDLE in Dutch are presented in chapter \ref{secResults}. \\

