\pagebreak
\chapter{Results}
\label{secResults}

This chapter describes the final results of applying SADDLE to the datasets Wikisim, WiRe, and gWikimatch. The research for this thesis consists of three iterations, which can be expanded in future research. The data structures are designed to be very flexible, so only a few changes had to be made to the data and the data preparation software. The most significant part of optimizing the SADDLE results applies to the "Modeling" phase.

\section{Baseline measurement}
\label{resultsBaselineMeasurement}
A baseline measurement is needed to answer the main research question: "How can hierarchical decomposition (into sections) and encoding improve on finding automated relations between documents in document sets compared to encoding all of the text of a document where the document set contains less than 10K documents?" \\


When determining the validity of relations between pairs of documents with the LHA algorithm, two primary hyperparameters can be tuned to improve performance. These hyperparameters are the embedding method and, second, the threshold used to determine whether a relation is valid (table \ref{tabHyperParameters}). Table \ref{tabWholeDocument} contains the maximum value of F1 per corpus and embedding algorithm. The maximum value of F1 was found by varying the threshold between 10 and 90 in steps of 10. These maximum values are based on the validation set and will be used in further experiments in which the test set is used for evaluation. The embedding algorithms "use" and "SBERT" are used in the second iteration of the results but are computed for the baseline measurement for clarity.\\


% tod:o cijfers nog aanpassen
\begin{table}[!ht]
    \centering
    \captionsetup{justification=centering}
    \begin{tabular}{l|l|l|l|l}
    \hline
        \textbf{Language} & \textbf{Corpus} & \textbf{Method} & \textbf{F1} & \textbf{Accuracy} \\ \hline
        en & gWikimatch & SBERT    & 0.68 & 0.58 \\ \hline
en & gWikimatch & Sent2Vec & 0.67 & 0.5  \\ \hline
en & gWikimatch & use      & 0.68 & 0.53 \\ \hline
en & gWikimatch & Word2Vec & 0.67 & 0.5  \\ \hline
en & Wikisim    & SBERT    & 0.8  & 0.79 \\ \hline
en & Wikisim    & Sent2Vec & 0.77 & 0.77 \\ \hline
en & Wikisim    & use      & 0.76 & 0.69 \\ \hline
en & Wikisim    & Word2Vec & 0.75 & 0.73 \\ \hline
en & WiRe       & SBERT    & 0.79 & 0.76 \\ \hline
en & WiRe       & Sent2Vec & 0.73 & 0.71 \\ \hline
en & WiRe       & use      & 0.76 & 0.74 \\ \hline
en & WiRe       & Word2Vec & 0.76 & 0.69 \\ \hline
nl & gWikimatch & SBERT    & 0.69 & 0.57 \\ \hline
nl & gWikimatch & Sent2Vec & 0.67 & 0.5  \\ \hline
nl & gWikimatch & use      & 0.68 & 0.6  \\ \hline
nl & gWikimatch & Word2Vec & 0.68 & 0.54 \\ \hline
nl & Wikisim    & SBERT    & 0.79 & 0.77 \\ \hline
nl & Wikisim    & Sent2Vec & 0.73 & 0.61 \\ \hline
nl & Wikisim    & use      & 0.83 & 0.8  \\ \hline
nl & Wikisim    & Word2Vec & 0.73 & 0.7  \\ \hline
nl & WiRe       & SBERT    & 0.79 & 0.75 \\ \hline
nl & WiRe       & Sent2Vec & 0.68 & 0.54 \\ \hline
nl & WiRe       & use      & 0.77 & 0.75 \\ \hline
nl & WiRe       & Word2Vec & 0.7  & 0.59 \\ \hline

    \end{tabular}
    \caption{The baseline F1 and accuracy values per corpus per language where the embedding method is applied to the whole Wikipedia article, without splitting the article in sections.}
    \label{tabWholeDocument}
\end{table}

The accuracy values in table \ref{tabWholeDocument} for gWikimatch are all near 0.50. The original gWikiMatch dataset contains 13.193 pairs, of which 6.643 are positive, being 50\%. That means roughly using a random value instead of an embedding gives the same results. Table \ref{tabSomewhatSimilar} gives a few examples of Wikipedia articles that are marked as somewhat similar but cannot be used in the training set because they are not considered valid document links for this thesis. As described in section \ref{sectWikiMatch}, gWikimatch contains articles from the group "similar/somewhat similar" or the group "not similar." The dataset that gWikimatch was based on makes a distinction between three groups: "similar," "somewhat similar," and "not similar."  This dataset is better suited for training because the "somewhat similar" document pairs can be filtered out. However, this dataset is not publicly available. Because training a neural network on an invalid set of examples can never result in a good algorithm, the gWikimatch dataset is not used for further results. \\

\begin{table}[!ht]
    \centering
    \captionsetup{justification=centering}
    \begin{tabular}{p{2.25cm}|p{4.75cm}|p{2.25cm}|p{4.75cm}}
    \hline
        \textbf{Article 1} & \textbf{Meaning} & \textbf{Article 2} & \textbf{Meaning} \\ \hline
        Branko Zebec & Branislav "Branko" Zebec was a Croatian footballer and manager who played for Yugoslavia (not a goalkeeper).    & Goalkeeper & A goalkeeper is a position in association football. It is the most specialised position in the sport  \\ \hline
        Matthew Spring & Matthew John Spring is an English semi-professional footballer (\textit{did not play for Bradford}) & Bradford City A.F.C & Bradford City Association Football Club is an English professional football club in Bradford.  \\ \hline
        Juan Bernat & Juan Bernat Velasco is a Spanish professional footballer  & Exhibition game & An exhibition game is a sporting event whose prize money and impact on the player's or the team's rankings is either zero or otherwise greatly reduced.  \\ \hline
        Batman and Robin (serial) & New Adventures of Batman and Robin, the Boy Wonder also known as simply Batman and Robin, is a 15-chapter serial released in 1949 by Columbia Pictures. & Congo Bill (serial) & Congo Bill (1948) is a Columbia movie serial based on the DC Comics character Congo Bill, later named Congorilla. \\ \hline
    \end{tabular}
    \caption{Examples of articles from gWikimatch marked as somewhat similar but not useful for this research.}
    \label{tabSomewhatSimilar}
\end{table}
\\
\section{Configuration}
The research was split into three iterations, where each iteration refines the previous iteration. The configuration that was used for these three iterations is described in this section. The next sections contain the specific configuration needed for that iteration.
\subsection{The neural network architecture}
An essential part of SADDLE is the neural network architecture that finds the proper semantic links using the similarity of sections (see \ref{phase3}). The architecture described by \citet{lecun1998} (section "One-hidden layer fully connected multilayer neural network") was used as an inspiration for SADDLE. This neural network recognizes handwritten digits in a grayscale image (MNIST). Figure \ref{imgMNIST} shows an example of these images. These grayscale images are similar to the heatmaps (figure \ref{imgHeatmaps}), and therefore, a similar neural network architecture is applicable for SADDLE.\\

\begin{figure}[h]
\centering
\captionsetup{justification=centering}

\includegraphics[scale=0.253]{images/mnist}
\caption{Examples of digits of the MNIST dataset}
\label{imgMNIST}
\end{figure}

The MNIST network consists of one fully connected layer of 300 hidden units and an output layer of 10 nodes. Because the number of samples available for the SADDLE is much lower than that of MNIST, a network architecture with fewer dimensions, typically one fully connected layer of 5 hidden units, was chosen.\\

The output value of the neural network serves two purposes. First, using a threshold, the output value determines whether the relation is valid. Second, the output value defines the significance of the semantic relations. A high value indicates a better semantic relation and, therefore, has a higher rank. This is useful in selecting the best semantic relations.\\

\subsection{Hyperparameters}
The results depend significantly on the values of the hyperparameters (see section \ref{secHyperParameters}). It is impossible to predict which configuration of hyperparameters will produce therefore, a grid search is used to find this configuration. The grid search process entails creating a grid of hyperparameter values for the model to test. Each combination of these values is assessed by the algorithm to determine the model's performance. The ultimate goal is to pinpoint the hyperparameter set that produces the most optimal results, measured by the F1 score. Table \ref{tabTestedParameters} contains the tested values for each hyperparameter, resulting in a total of $2 \cdot 3  \cdot 4  \cdot 2  \cdot 4  \cdot 4  \cdot  2  \cdot  4\cdot4  \cdot  3 = 73,728$ configurations. \\
 

\begin{table}[!ht]
    \centering
    \captionsetup{justification=centering}
    \begin{tabular}{l|l}
    \hline
        \textbf{Parameter} & \textbf{Values} \\ \hline
        Language & en, nl\\ \hline
        Corpus & Wikisim, WiRe\\ \hline
        Method & Word2Vec, Sent2Vec, SBERT, use\\ \hline
        NN & 10, 20, 30\\ \hline
        Sim & 30, 50, 70, 80\\ \hline
        Maxdoc & 20, 30, 40, 50\\ \hline
        Sections & 9, 12\\ \hline
        Batch size & 20, 50, 100, 200 \\ \hline
        Epochs & 10, 60, 100, 200\\ \hline
        Folds & 2, 5 \\ \hline
        Learning rate & 0.001, 0.01, 0.1\\ \hline
    \end{tabular}
    \caption{The values of the hyperparameters tested to find the optimal results.}
    \label{tabTestedParameters}
\end{table}

First, to find the optimal results (using the F1 value as an indicator) for every language/corpus/method combination, all results were assembled by a Python script in a Pandas\footnote{https://pandas.pydata.org} data frame. This script can export the data frame to Excel, a tab-separated text file, or a database. The database export is the most helpful because the number of records is too high to handle manually, and the database can easily be queried via SQL. PostgresSql\footnote{https://www.postgresql.org/} was chosen as the Relational Database Management System (RDBMS) because it is free to use, has excellent query features, and is supported by Pandas for export. Appendix G contains a selection of the queries resulting in this chapter's table values. Because, in database terms, the number of records is limited, and the queries only have to be run a limited number of times, no effort was put into adding indexes or doing other optimizations.\\

{
\begin{table}[!ht]
	\parbox{.45\linewidth} {
    \centering
    \captionsetup{justification=centering}
    \begin{tabular}{l|l|l}
    \hline
        \textbf{Parameter} & \textbf{English} & \textbf{Dutch} \\ \hline
        NN & 70 & 50 \\ \hline
        Sim & 30 & 40\\ \hline
        Maxdoc & 30 & 20\\ \hline
        Sections & 12 & 9\\ \hline
        Batch size & 20 & 100 \\ \hline
        Epochs & 200 & 200\\ \hline
        Folds & 5 & 5\\ \hline
        Learning rate & 0.1 & 0.1\\ \hline
    \end{tabular}
    \caption{The optimal configuration of hyperparameters used for the results for iteration two.}
    \label{tabChoosenHyperparameters12}
    }
\parbox{.45\linewidth}{
\centering
    \centering
    \captionsetup{justification=centering}
    \begin{tabular}{l|l|l}
    \hline
        \textbf{Parameter} & \textbf{English} & \textbf{Dutch} \\ \hline
        NN & 80 & 80 \\ \hline
        Sim & 50 & 50\\ \hline
        Maxdoc & 10 & 10\\ \hline
        Sections & 12 & 12\\ \hline
        Batch size & 50 & 100 \\ \hline
        Epochs & 10 & 200\\ \hline
        Folds & 5 & 5\\ \hline
        Learning rate & 0.01 & 0.001\\ \hline
    \end{tabular}
    \caption{The optimal configuration of hyperparameters used for the results for iteration three.}
    \label{tabChoosenHyperparameters3}
    }
\end{table}
\\ 

Four configurations were extracted: two for phases one and two (English and Dutch) and two for phase three (English and Dutch). It is valid that these configurations can differ because they use a different neural network or the datasets differ. The hyperparameters for phases one and two are shown in table \ref{tabChoosenHyperparameters12}, and those for phase three are shown in table \ref{tabChoosenHyperparameters3}. The values of the parameters were read from table \ref{tabMaxF1} of Appendix G. Because SBERT gives the best algorithm performance and WiRe is the largest dataset, the hyperparameters corresponding to the highest value for F1 of the combination of SBERT and WiRe were chosen.\\

\subsection{Training set and test set}
The WiRe and Wikisim datasets have been split into a training and a test set (see section \ref{secTrainingNeuralNetworks}). Due to the small data size, using a separate validation set is not feasible. Instead, K-fold cross-validation is used to determine the best hyperparameters properly while allowing the training data's optimal use. The pairs in the training set are used for training only. The pairs in the test set are not being used for training; they are only used to measure the quality of the neural network. Because the number of items in the datasets is relatively low, the choice was made to use as many pairs for training as possible and leave enough pairs for testing purposes: 85\% of the dataset is used as a training set and 15\% as a test set. The distribution of similar and dissimilar pairs in the training and test set must be almost equal. Functions of the Scikit-learn Python module were used to establish the proper distribution. The files containing the sets are saved with the corpus to ensure the training and test sets are fixed throughout the process. The number of pairs in each set is given in table \ref{tabTestDistr}. The items in the dataset can have two values: a value of 1 denotes a similar article pair, whereas a value of 0 denotes a dissimilar article pair. 

\begin{table}[!ht]
    \centering
    \captionsetup{justification=centering}
    \begin{tabular}{l|l|l|l|l|l|l|l}
    \hline
        \textbf{Lang} & \textbf{Corpus} &  \textbf{Test} & \textbf{Test-zeros} & \textbf{Test-ones} & \textbf{Train} & \textbf{Train-zeros} & \textbf{Train-ones} \\ \hline
en & Wikisim & 39 & 19 & 20 & 217 & 110 & 107  \\ \hline
en & WiRe & 64 & 34 & 30 & 362 & 174 & 188 \\ \hline
nl & Wikisim & 30 & 14 & 16 & 173 & 81 & 92 \\ \hline
nl & WiRe & 45 & 22 & 23 & 265 & 129 & 136 \\ \hline

\end{tabular}
    \caption{Distribution of zeros and ones in the test and training sets. The value of a pair can either be 0 (not similar) or 1 (similar).}
    \label{tabTestDistr}
\end{table}

\section{Iteration 1}
\label{resultsIteration1}
The first iteration in optimizing SADDLE is implementing the embedding algorithms of the LHA algorithm and a simple neural network. For documents in the English language, the same embedding algorithms as the LHA algorithm can be used for SADDLE: Average Word2Vec and Sent2Vec, where the pre-trained model for Word2Vec is "GoogleNews vectors negative300" and for Sent2Vec, the model used is "Wiki Unigrams". The pre-trained models are part of Python modules for Gensim\footnote{https://radimrehurek.com/gensim} and Sent2Vec\footnote{https://github.com/epfml/Sent2Vec} respectively.\\

Unfortunately, the Python modules do not contain pre-trained models for the Dutch language. Sent2Vec has no pre-trained Dutch model available, creating the suboptimal situation where a model trained in English is used for documents written in Dutch. \citet{nlWord2Vec} made pre-trained Word2Vec models available for documents written in Dutch. For this thesis, the model "combined-320" was chosen.\\

All hyperparameter configurations were tested, resulting in the values shown in Appendix G. K=5 was used for the K-Fold Cross Reference because it had better, or almost equal, results than K=2.\\

The neural network used in phase 3 has a hidden layer with 5 nodes with ReLu activations and one output layer with sigmoid activation (see section \ref{phase3}). Table \ref{tabFirstIteration} contains the results of the first iteration. 
 \begin{table}[!ht]
    \centering
    \captionsetup{justification=centering}
    \begin{tabular}{l|l|l|l|l|l|l}
    \hline
        \textbf{} & \textbf{Corpus} & \textbf{Method} &  \textbf{F1} & \textbf{Accuracy} & \textbf{Baseline F1} & \textbf{Baseline Accuracy} \\ \hline
en & Wikisim & Sent2Vec & 0.27 & 0.41 & 0.77 & 0.77 \\ \hline
en & Wikisim & Word2Vec &  0.70 & 0.55 & 0.75 & 0.73 \\ \hline
en & WiRe & Sent2Vec & 0.43 & 0.58 & 0.73 & 0.71 \\ \hline
en & WiRe & Word2Vec &  0.06 & 0.55 & 0.76 & 0.69 \\ \hline
nl & Wikisim & Sent2Vec &  0.80 & 0.72 & 0.73 & 0.61 \\ \hline
nl & Wikisim & Word2Vec &  0.33 & 0.29 & 0.3873 & 0.70 \\ \hline
nl & WiRe & Sent2Vec & 0.00 & 0.50 & 0.68 & 0.54 \\ \hline
nl & WiRe & Word2Vec &  0.40 & 0.47 & 0.70 & 0.59 \\ \hline
\end{tabular}
    \caption{Results of the first iteration, all F1 and accuracy scores are lower than in the baseline.}
    \label{tabFirstIteration}
\end{table}

 
 \section{Iteration 2}
 \label{secIteration2}
 The second iteration uses text embedding algorithms designed after LHA was developed. The idea behind this choice is that newer algorithms produce better embeddings, improving the document relations' quality. For this iteration, the algorithms Universal Sentence Encoder (USE) \citep{cer2018universal} and Sentence BERT \citep{reimers2019sentence} (SBERT) are used. USE uses a pre-trained model based on DAN and trained on documents in the English language\footnote{https://tfhub.dev/google/universal-sentence-encoder/4} and a multilingual model trained in 16 languages, including Dutch\footnote{https://tfhub.dev/google/universal-sentence-encoder-multilingual/}. The SBERT model uses an English\footnote{https://huggingface.co/sentence-transformers/all-mpnet-base-v2} and a multilingual\footnote{https://huggingface.co/sentence-transformers/distiluse-base-multilingual-cased-v1} model, too.\\
 
 The results using these algorithms are summed up in table \ref{tabSecondtIteration}. When the values of this iteration are compared to the results of the first iteration (table \ref{tabFirstIteration}) it can be concluded that the results do not improve using these algorithms, where SBERT performs better than USE.
 
  \begin{table}[!ht]
      \centering
    \captionsetup{justification=centering}
    \begin{tabular}{l|l|l|l|l|l|l}
    \hline
        \textbf{Lang} & \textbf{Corpus} & \textbf{Method} &  \textbf{F1} & \textbf{Accuracy} & \textbf{Baseline F1} & \textbf{Baseline Accuracy} \\ \hline
en & Wikisim & SBERT & 0.63 & 0.50 & 0.80 & 0.79 \\ \hline
en & Wikisim & use &  0.00 & 0.45 & 0.76 & 0.69 \\ \hline
en & WiRe & SBERT & 0.70 & 0.67 & 0.79 & 0.76 \\ \hline
en & WiRe & use &  0.62 & 0.50 & 0.76 & 0.74 \\ \hline
nl & Wikisim & SBERT & 0.35 & 0.46 & 0.79 & 0.77 \\ \hline
nl & Wikisim & use & 0.40 & 0.36 & 0.83 & 0.80 \\ \hline
nl & WiRe & SBERT & 0.34 & 0.58 & 0.79 & 0.75 \\ \hline
nl & WiRe & use & 0.58 & 0.42 & 0.77 & 0.75 \\ \hline
\end{tabular}
    \caption{Results of the second iteration, all F1 and accuracy scores are lower than in the baseline.}
    \label{tabSecondtIteration}
\end{table}


 \section{Iteration 3}
 \label{secIteration3}

The drawback of using the neural network in iteration one and iteration two is that the input vector size is relatively large (section \ref{phase3}. When the dimensions of the input vector increase, the number of parameters in the neural network increases significantly. This results in the training set to train the neural network requiring more validated examples, which is unwanted because of the increased costs of creating a training set manually.\\ One way to decrease the number of dimensions in the input vector is to use general dimension reduction algorithms, which are not optimal because they do not consider the document structure. \\

Another way to decrease the number of dimensions is to use statistical functions to create a small vector representing the document pair. These functions use the relations between matched sectors in the document pair. The statistical vector used in iteration three can be represented by: $$\vec{v} = [c, a, s, d]$$

$c$ is the total number of sections with similarity above a certain threshold divided by the total number of sections. Parameter $a$ is the average number of sections whose similarity is above the threshold. Finally, parameters $s$ and $d$ represent the ratio of sections that match in the first part of the source and destination document, respectively. The idea behind this is that sections at the beginning of a document provide more information about the document than those at the end.
The results using this reduced input vector are represented in table \ref{tabThridIteration}.\\

  \begin{table}[!ht]
    \centering
    \captionsetup{justification=centering}
    \begin{tabular}{l|l|l|l|l|l|l}
    \hline
        \textbf{Lang} & \textbf{Corpus} & \textbf{Method} &  \textbf{F1} & \textbf{Accuracy} & \textbf{Baseline F1} & \textbf{Baseline Accuracy} \\ \hline
en & Wikisim & SBERT & 0.00 & 0.51 & 0.80 & 0.79 \\ \hline
en & Wikisim & Sent2Vec & 0.66 & 0.49 & 0.77 & 0.77 \\ \hline
en & Wikisim & use & 0.00 & 0.51 & 0,76 & 0.69 \\ \hline
en & Wikisim & Word2Vec &  0.66 & 0.49 & 0.75 & 0.73 \\ \hline
en & WiRe & SBERT & 0.00 & 0.48 & 0.79 & 0.76 \\ \hline
en & WiRe & Sent2Vec & 0.68 & 0.52 & 0.73 & 0.71 \\ \hline
en & WiRe & use & 0.68 & 0.52 & 0.76 & 0.74 \\ \hline
en & WiRe & Word2Vec & 0.00 & 0.48 & 0.76 & 0.69 \\ \hline
nl & Wikisim & SBERT & 0.66 & 0.50 & 0.79 & 0.77 \\ \hline
nl & Wikisim & Sent2Vec & 0.66 & 0.50 & 0.73 & 0.61 \\ \hline
nl & Wikisim & use & 0.66 & 0.50 & 0.83 & 0.80 \\ \hline
nl & Wikisim & Word2Vec & 0.66 & 0.50 & 0.73 & 0.70 \\ \hline
nl & WiRe & SBERT & 0.68 & 0.52 & 0.79 & 0.75 \\ \hline
nl & WiRe & Sent2Vec & 0.00 & 0.48 & 0.68 & 0.54 \\ \hline
nl & WiRe & use & 0.68 & 0.52 & 0.77 & 0.75 \\ \hline
nl & WiRe & Word2Vec & 0.00 & 0.48 & 0.70 & 0.59 \\ \hline
\end{tabular}
    \caption{Results of the third iteration, all F1 and accuracy scores are lower than in the baseline.}
    \label{tabThridIteration}
\end{table}

 \section{Analyzing the resulted neural network}
 \label{secResultingNeuralNetwork}

Neither of the phases produces better results than the baseline. It is interesting to know on what basis the neural network calculates the relation between two documents or whether it can easily improved. This information can be used for intuitive validation.\\

\subsection{Using artificial data}
A Python script was written that creates vectors with artificial values. The values in these vectors can either be high (0.8-1.0), medium(0.2-0.8), or low(0-0.2). The vector is initialized with low values. The document is divided into three locations: the start, middle, and end. Each location can contain sections related to one or more sections in the other document. The relations can be strong (a high value) or weak (a low value). The Python script combines all parameters into sets, evaluated by the neural network, resulting in a final score. This score is an indication of the strength of the document relation.\\

Tables \ref{tabInsightModel1} and  \ref{tabInsightModel2} in Appendix F contain the resulting scores. The scores show the following trends for selecting a higher score (some are obvious):
\begin{itemize}
  \item A strong relation between sections.
  \item A higher number of related sections.
  \item A higher number of sections to which a section is related.
  \item The position within the document matters. The relation is more important when the location is nearer to the top of the document.
\end{itemize}

\subsection{Overfitting}
The F1 and accuracy values during training are higher than those during testing. This probably means the network overfits the data (see section \ref{secTrainingNeuralNetworks}). The reasons for overfitting mentioned in this section seem to apply to this research. The most obvious reason seems to be that the dataset does not contain enough data. Next to the small dataset, the complexity of the neural network for the data available can also be a problem. \\

The many parameters used during the grid search can also be problematic. "The basic point is that if data has been used to optimise the model in any way, then it will give an optimistically biased performance estimate. How biased it is depends on how hard you try to optimise the model (how many feature choices, how many hyper-parameters, how fine a grid you use in gridsearch etc.) and the characteristics of the dataset. In some cases it is fairly benign.", Stack Exchange post\footnote{https://stats.stackexchange.com/questions/438651/overfitting-the-validation-set} which uses \citep{Nested-cross-validation} as a reference.\\

This is illustrated by table \ref{tabAverageF1}. This table contains the average F1 and accuracy during training to the baseline average. As can be seen in the table, almost all training values are higher than the baseline. Exceptions to this are some of the networks of iteration three. The results of the third iteration are low, as described in the previous paragraphs, so this was to be expected. Some hyperparameter configurations are very bad and were left out of the average computation. Only F1 values higher than 0.5 were used for the average. The average baseline was chosen to compare the averages because there is no default configuration of the hyperparameters.\\

  \begin{table}[!ht]
    \centering
    \captionsetup{justification=centering}
    \begin{tabular}{l|l|l|l|l|l|l|l|l}
    \hline
        \textbf{Lang} & \textbf{Corpus} & \textbf{Method} & \textbf{Network} & \textbf{F1} & \textbf{Acc} & \textbf{Base F1} & \textbf{Base Acc} & \textbf{Cmp} \\ \hline
en & wikisim & SBERT & plain & 0.8 & 0.8 & 0.8 & 0.8 & + \\ \hline
en & wire & SBERT & plain & 0.8 & 0.8 & 0.8 & 0.8 & + \\ \hline
en & wikisim & Sent2Vec & plain & 0.7 & 0.7 & 0.8 & 0.8 & + \\ \hline
en & wire & Sent2Vec & plain & 0.7 & 0.6 & 0.7 & 0.7 & + \\ \hline
en & wikisim & USE & plain & 0.8 & 0.8 & 0.8 & 0.8 & + \\ \hline
en & wire & USE & plain & 0.7 & 0.7 & 0.8 & 0.7 & + \\ \hline
en & wikisim & Word2Vec & plain & 0.7 & 0.7 & 0.8 & 0.7 & + \\ \hline
en & wire & Word2Vec & plain & 0.7 & 0.6 & 0.8 & 0.7 & + \\ \hline
en & wikisim & SBERT & stat & 0.6 & 0.5 & 0.8 & 0.8 & + \\ \hline
en & wire & SBERT & stat & 0.7 & 0.5 & 0.8 & 0.8 & + \\ \hline
en & wikisim & Sent2Vec & stat & 0.6 & 0.5 & 0.8 & 0.8 & - \\ \hline
en & wire & Sent2Vec & stat & 0.7 & 0.5 & 0.7 & 0.7 & + \\ \hline
en & wikisim & USE & stat & 0.6 & 0.5 & 0.8 & 0.8 & + \\ \hline
en & wire & USE & stat & 0.7 & 0.5 & 0.8 & 0.7 & + \\ \hline
en & wikisim & Word2Vec & stat & 0.6 & 0.5 & 0.8 & 0.7 & - \\ \hline
en & wire & Word2Vec & stat & 0.7 & 0.5 & 0.8 & 0.7 & - \\ \hline
nl & wikisim & SBERT & plain & 0.8 & 0.8 & 0.8 & 0.8 & + \\ \hline
nl & wire & SBERT & plain & 0.8 & 0.7 & 0.8 & 0.8 & + \\ \hline
nl & wikisim & Sent2Vec & plain & 0.7 & 0.7 & 0.7 & 0.6 & + \\ \hline
nl & wire & Sent2Vec & plain & 0.6 & 0.6 & 0.7 & 0.6 & + \\ \hline
nl & wikisim & USE & plain & 0.8 & 0.8 & 0.8 & 0.8 & + \\ \hline
nl & wire & uUSEse & plain & 0.7 & 0.7 & 0.8 & 0.8 & + \\ \hline
nl & wikisim & Word2Vec & plain & 0.7 & 0.6 & 0.7 & 0.7 & - \\ \hline
nl & wire & Word2Vec & plain & 0.6 & 0.6 & 0.7 & 0.7 & - \\ \hline
nl & wikisim & SBERT & stat & 0.6 & 0.5 & 0.8 & 0.8 & + \\ \hline
nl & wire & SBERT & stat & 0.7 & 0.5 & 0.8 & 0.8 & + \\ \hline
nl & wikisim & Sent2Vec & stat & 0.6 & 0.5 & 0.7 & 0.6 & + \\ \hline
nl & wire & Sent2Vec & stat & 0.7 & 0.5 & 0.7 & 0.6 & + \\ \hline
nl & wikisim & USE & stat & 0.6 & 0.5 & 0.8 & 0.8 & + \\ \hline
nl & wire & USE & stat & 0.7 & 0.5 & 0.8 & 0.8 & + \\ \hline
nl & wikisim & Word2Vec & stat & 0.6 & 0.5 & 0.7 & 0.7 & - \\ \hline
nl & wire & Word2Vec & stat & 0.7 & 0.5 & 0.7 & 0.7 & - \\ \hline


\end{tabular}
    \caption{All models' average F1 and accuracy compared to the average baseline with an F1 > 0.5. The last column contains a '+' when the networks F1 is greater than that of the baseline and a '-' otherwise.}
    \label{tabAverageF1}
\end{table}


For this research, it is not easy to mitigate overfitting. Finding new data has proven to be difficult. It is not easy to create augmented data. When using regularization, the little data available becomes even less. Pruning is attempted during iteration three. The number of features decreased dramatically during this iteration, but this did not improve the results.\\




