\pagebreak
\chapter{Data understanding and data preparation}
\label{chapterUnderstandingPreperation}

Phase 2 of the Crisp-DM method concerns the understanding of the data. Understanding the data means a dataset must be found or created corresponding to the research aim. The dataset can be used to answer the subquestion "What is a representative corpus for training?" and part of the subquestion "What is a representative document set for validating and what metrics can be used?".\\ % todo Controleren of de vragen nog gelijk zijn aan wat hier staat

The next phase in the CRISP-DM method represents the preparation of the data. During the modeling phase (described in chapter \ref{secRCIV} it is convenient to be independent of the data's original format. Therefore the datasets will be converted to a common format to simplify the data processing. \\

%todo: blijft dit nog staan? Because the common format is based on dividing documents into sections, part of the subquestion "What method can be used for determining the hierarchy of a document in sections and paragraphs?" will be answered.\\ 
% todo Hier ook: Controleren of de vragen nog gelijk zijn aan wat hier staat



This chapter describes the understanding of the data and the data preparation of the Crisp-DM method because they are tightly linked and implemented in the "Corpus" project, as depicted in figure \ref{imgtotalarch}. Firstly, a description is given of the datasets that this thesis uses, sections \ref{secApproach} and \ref{secUsedDatasets}. Secondly, section \ref{secDataPreparation} describes additional data preparation steps.

\section{}
% todo: tekst en titel voor deze sectie nog bepalen
\subsection{Datasets used in research}
\label{secDatasetsResearch}
Although only a little research was performed, the datasets used for existing research were a good starting point for finding datasets for this thesis. Therefore, existing research into long-form document comparison was analyzed to determine whether the used datasets were suitable for this thesis. Table \ref{tabDatasets} gives an overview of datasets used or created for the document-to-document comparison in recent papers.\\


\begin{table}[h!tbp]
    \centering
    \begin{tabular}{l|l|l|l|l|l|l}
    	\textbf{Paper} &  \textbf{OC} & \textbf{AAN}  
    	& \textbf{S2ORC}  & \textbf{CNSE/CNSS} & \textbf{Wikipedia} & \textbf{GWikiMatch}  \\
		\hline
        \citet{jiang2019semantic} & & used & & & used &   \\
		\hline
        \citet{512Tokens} & & used & &  & used & created   \\
		\hline
        \cite{zhou2020multilevel} & used & used & used &  &  &    \\ 
		\hline
        \citet{Pang2021}& used & used & used & used &  \\ 
		\hline
    \end{tabular}
    \caption{Datasets used by the research on long-document to long-document comparison, the GWikiMatch dataset was created by the research.}
  	\label{tabDatasets}
\end{table}

%todo: dataset referenties nog toevoegen. voor voorbeeld zie: https://xuhuizhou.github.io/Multilevel-Text-Alignment/

Every dataset mentioned in table \ref{tabDatasets} has been evaluated in terms of its suitability for this thesis. Selection criteria for finding suitable datasets were: the availability in English and at least one other language (preferably Dutch), and the text must be separated into sections.\\

 The Semantic Scholar Open Corpus (OC) \citep{Bhagavatula2018}, the CL Anthology Network Corpus (AAN) \citep{radev}, and the Semantic Scholar Open Research Corpus (S2ORC) \citep{lo2019} corpora contain scientific documents and are therefore only available in English. The document links are created by using scientific citations within the papers. Because the corpus license is limited, the OC does not contain the whole text but only the title, abstract, and other small parts. In this thesis, entire documents are needed to analyze the structure, which makes the OC dataset unsuitable. However, the AAN and S2ORC datasets do not have this restriction and can be used for this thesis as a reference for text in the English language. The S2ORC dataset can be used for the alignment of document sections as described in \citet{zhou2020multilevel}; the documents in AAN, however, are not split into sections. The Chinese News Same Event dataset (CNSE) and the Chinese News Same Story dataset (CNSS) released in \citet{Liu2019} are both in the Chinese language and, therefore, not suitable for this thesis. \\

Another approach is to generate datasets based on Wikipedia articles, as was done by \citet{jiang2019semantic} and \citet{512Tokens}. This approach has several advantages compared to using the other datasets. Firstly, it allows composing a set of Wikipedia articles related to a specific topic to simulate a knowledge base. Secondly, when choosing a multilingual ontology, datasets can be created in multiple languages because Wikipedia is multilingual. The latter helps answer the sub-research question, "To what extent can this method be extended to a different language than English (for example, Dutch)?" %Todo: klopt deze research vraag nog?
Lastly, Wikipedia articles don’t have license restrictions that prevent using them as a dataset. \citep{512Tokens} contains a reference to a generated dataset based on Wikipedia that was manually checked, the GWikiMatch dataset. \\

%For this thesis, a selection was made for the datasets. Firstly, the S2ORC dataset was chosen because the articles were split into sections; another reason to select this dataset is that the algorithm's results can be compared to the algorithms that use the S2ORC dataset and are mentioned in table \ref{tabDatasets}. Secondly, to simulate a knowledge base as well as possible, a selection of Wikipedia articles about a topic was made in English and Dutch. The process for creating this dataset is described in section \ref{sectWikidata}.
%
% Thirdly, all articles containing the GWikiMatch linkset are copied from Wikipedia in English and Dutch. The main reason this set was chosen is that the links are manually checked..... %todo verder afmaken 

\section{Used datasets}
\label{secUsedDatasets}
Using the considerations mentioned in section \ref{secApproach}, the following datasets were chosen to be used in this thesis:
\begin{itemize}
  \item \textit{S2ORC}, a dataset containing scientific papers written in the English language
  \item \textit{WikiData - EN}, a selection of Wikipedia articles about a topic in the English language
  \item \textit{WikiData - NL}, a selection of Wikipedia articles about a topic in the Dutch language
  \item \textit{GWikiMatch - EN}, manually curated links between a subset of Wikipedia articles in the English language
  \item \textit{GWikiMatch - NL}, the same articles as \textit{GWikiMatch - EN} but then in the Dutch language
\end{itemize}
Each of the datasets will be described in the following paragraphs.

\subsection{The S2ORC dataset}
\label{secS2orc}
For this thesis, version 2020-07-05 of the S2ORC dataset is used. It contains the metadata of about 136 million papers and the full text of 81 million papers. The full text has been parsed from PDF and XML into a generic JSON format that has the text divided into sections, and the citations have been resolved to usable paper ids. That means we can use this set or create links between documents and between sections and documents.\\

The S2ORC dataset has to be preprocessed to be used for this thesis. First, the citation texts will be removed from the section text to prevent the algorithm from selecting citation texts for creating links. Second, because this thesis aims to create an algorithm that uses a small set of documents, a small portion of the S2ORC dataset will be selected. The papers in S2ORC are grouped by the field of study. This thesis selects a small field of study with a limited amount of papers: about 10.000 documents with history as a subject.\\

\subsection{The WikiData datasets}
\label{sectWikidata}

Wikidata \citet{vrandevcic2014wikidata} and DBPedia \citet{dbpedia} are knowledge graphs linked to Wikipedia articles and potentially be used for generating suitable datasets. The DBPedia knowledge graph is created by extracting structured content from Wikipedia information. On the other hand, the knowledge graph created by the Wikidata project serves as an information source for projects like Wikipedia. This means that DBPedia uses Wikipedia as a source, and Wikidata is a source for Wikipedia. This means that Wikidata is better suited for creating datasets because the text from Wikidata does not necessarily have to be contained in the final Wikipedia article. The Wikidata knowledge graph is less dependent on the information and structure of the Wikipedia article than DBPedia therefore, the eventual dependence between Wikidata and the algorithm to be created will be less than the dependence between Wikipedia and DBPedia.\\

Wikidata is a knowledge graph that is multilingual by design and is published under legal terms that allow the broadest possible reuse \citet{vrandevcic2014wikidata}. Information from Wikidata can be represented as semantic triples in the form of Resource Description Format (RDF) triples \citet{erxleben}. RDF triples are a standardized way to describe information. They are in the form \texttt{<subject, predicate, object>}. For example, the fact that a dog can make a barking sound can be represented by the triple \texttt{ <dog, produced sound, bark>}. Because Wikidata is multilingual by design, ids represent the elements of the triple that can be translated into a description in one of the many languages supported by Wikidata. The previous example is described by the Wikidata RDF triple \texttt{<wd:Q144, wdt:P4733, wd:Q38681>}.\\ 

The dataset used in the research has to match the content of a knowledge base as closely as possible. This means that the dataset must at least have two essential properties. Firstly, the dataset must contain content about one or multiple closely related topics. Secondly, the dataset must contain a substantial set of documents, but not more than 10K. The \texttt{subsclass of}-property will be used to obtain documents about a topic. \\

The standard query language for RDF-stores is SPARQL\footnote{https://www.w3.org/TR/sparql11-query/} for which Wikidata provides an endpoint\footnote{https://query.wikidata.org/sparql}. The endpoint has two main limitations. Firstly, one client (user agent + IP) is allowed 60 seconds of processing time. Secondly, one client is permitted 30 error queries per minute. A recursive query is used to select all Wikipedia articles concerning a topic to limit the number of queries to the endpoint. The framework of the query is contained in appendix \ref{appendix:Wikipedia}\\

The articles are retrieved from a Wikimedia dump to avoid using the Wikipedia API because the number of queries via the API is limited. The dumps used in this thesis were created in May 2022. %todo: de dataset is van een andere datum, dit nog aanpassen 
\\

 Semantic links between articles are constructed based on two criteria. The criterium of document similarity, as described in \citet{jiang2019semantic}, is based on the Jaccard similarity \citet{Jaccard1912} between the outgoing links of two Wikipedia articles. The assumption is made that similar documents have similar sets of outgoing links. Document pairs with a Jaccard index greater than 0.5 %todo: dit getal nog nader bepalen (Jacard-index-drempel)
  are considered positive examples. To avoid using links twice, the document with the shortest Url is considered the source document. %todo: dit heb ik nu niet geïmplementeerd, als ik dat ook niet doe, dan kan deze en de volgende zin weg 
   For creating links between sections of articles, only the Jaccard distance is used for creating semantic links because Wikipedia does not contain direct links between sections of articles.

\subsection{GWikiMatch}
\label{sectWikidata}
As part of their work, \citet{512Tokens} released a benchmark that researchers can use for comparing different document-matching methods. The dataset is called GWikiMatch: "a benchmark dataset for the long-form document matching task, which is to estimate the content semantic similarity between document pairs." The dataset is based on articles from English Wikipedia. Although the dataset was generated in 2021, it is used in this thesis because changes in the text articles probably do not affect the relationship between articles.\\

The dataset was generated in four steps. First, an article graph was constructed from Wikipedia, on which several algorithms were applied to generate article pairs. Second, each document pair was annotated by three human raters on a crowdsourcing platform. The Majority vote rating was used as the content similarity label, which was either 0 (not similar), 1 (somewhat similar), or 2 (strongly similar). A total of 11K document pairs were annotated. Third, by using negative sampling, the dataset was enlarged and made more balanced. After finishing the third step, the assigned labels to the document pairs were either 0 (not similar) or 1 (somewhat similar). Last, controversial topics were removed by further human annotation. The dataset and details of the generation of the dataset can be found on GitHub\footnote{https://github.com/google-research/google-research/tree/master/gwikimatch}. Unfortunately, the document pairs humans annotated in the second step are not publicly available. These document pairs could have been used to identify links that were strongly similar and not similar, leaving out the somewhat similar links. That would have made the dataset more unambiguous.  \\

For this thesis, a script was created that translates the names of the articles to WikiData labels. This translation is done for two reasons. First, the software created for extracting articles from Wikipedia dumps can be reused using these labels. Second, using the WikiData labels, the articles can be translated into Dutch. Thus a Dutch version of the GWikiMatch dataset can be created. The script assumes that when article $A$ is related to article $B$, this relationship can be reversed, so that article $B$ is also related to article $A$. Table \ref{tabGWikiMatch} contains the number of articles and valid and invalid links per language. Because not every English Wikipedia article is available in the Dutch Wikipedia, the values for Dutch are significantly lower. Nevertheless, the number of articles for both languages is sufficient to simulate a small knowledge base.




\begin{table}[h!tbp]
    \centering
    \begin{tabular}{l|l|l}

\textbf{Number of} & \textbf{English} & \textbf{Dutch}  \\
		\hline
Articles      & 18700   & 7972   \\
		\hline
Valid links   & 13091   & 5715   \\
		\hline
Invalid links & 13276   & 5701 \\
		\hline
    \end{tabular}
    \caption{The number of articles and valid and invalid links per language.}
  	\label{tabGWikiMatch}
\end{table}



\section{Data preparation}
\label{secDataPreparation}

% Todo hier mee verder gaan!
For every dataset described in section \ref{secUsedDatasets}, a script was developed to perform the needed preprocessing and translate the data to a common file format. For several reasons, XML was chosen as the common file format. Firstly, it can easily be created and read because of the availability of software libraries for almost every computer language, including Python. Secondly, it is easily readable by humans. Lastly, the XML format does not restrict the structure to be used.\\
The scripts are publicly available on GitHub\footnote{https://github.com/johnstegink/wikidata-corpus} % todo: link nog aanpassen, dit is vast nog niet goed
. The code is explained with comments inside the code. The accompanying readme file describes the parameters of the scripts for generating the datasets. For convenience, this readme file is included in appendix \ref{appendix:ReadMe}.\\

Except for S2ORC, the corpora use Wikipedia as the text source. The Wikipedia texts must be cleaned before the texts are included in the corpus. The cleaning of the text is done while generating the XML and is not implemented as a separate step. It is started by removing text components that are not common to texts of knowledge bases. Articles containing less than 128 characters are removed because it was found that these articles are either redirecting to another article or contain too little information. Tables in the text contain a lot of information, albeit in a structured manner that is helpful for human readers. This structure, however, is hard to read for NLP algorithms and is therefore removed. This is because the focus of this thesis is on sentences and paragraphs. More or less, the same applies to enumerations. The article \textit{"Merrick (surname)"}, for example, contains a list of (famous) people having the surname "Merrick." By making random spot checks it was found that most enumerations in Wikipedia do not contain full sentences but consist of a few terms. For that reason, enumerations are removed too.\\

Wikipedia articles contain explicit metadata that is not generally contained in texts. This surplus of information 
must be removed to avoid the algorithm using this information for the creation of semantic links. That would make the algorithm not suited to be used for plain text. For this reason, hyperlinks are simplified so that the target is removed, and only the link's text is maintained. Finally, sections containing references and categories are removed from the text entirely.\\

\section{Validation Metrics}
\label{secValidationMetrics}
\subsection{F1 Score}
\label{F1Score}
Evaluation metrics quantify the quality of the semantic links generated by an algorithm. They can be used to compare the quality of various algorithms and help improve an algorithm while it is in development. The F1 metric \cite{forman2003} is an often-us evaluation metric in NLP research. It is a measure of an algorithm’s accuracy on a dataset, represented by the formula:
$$F1 = \frac{ 2 . recall . precision}{recall + precision}$$
$$precision = \frac{|\{relevant\ documents\}\  \cap\  \{retrieved\ documents\}|}{|\{retrieved\ documents\}|} $$ \\
$$recall = \frac{|\{relevant\ documents\}\  \cap\  \{retrieved\ documents\}|}{|\{relevant\ documents\}|} $$ \\
In a classification context, values for precision and recall can be calculated with the following formulas:
$$precision = \frac{tp}{tp + fp},\ \ \ \ recall = \frac{tp}{tp + fn}$$ where \textit{tp} denotes the \textit{true positives}, \textit{fp} the \textit{false positives} and.  \textit{fn} the \textit{false negatives}.\\

\subsection{Ranking}
\label{ranking}
The generation of semantic links can be viewed as a classification problem, meaning that a result that is both present in the generated links and the validation data is considered a true positive. However, it is a ranking problem with the most relevant semantic links at the top. \cite{Rajaram2003} argues that "If the ranking problem is posed as a classification problem, then the inherent structure present in ranked data is not made use of and hence generalization ability of such classifiers is severely limited.".   The generation of semantic links can also be viewed as a ranking problem for search, with the document being the query and the semantic links representing the search results. \\

By using precision at K (P(K)), \cite{Agichtein2006} an F1 score can be calculated for this ranking problem. P(K) reports the fraction of documents ranked in the top K results labeled as relevant. The position of relevant documents within the top K is irrelevant. This can be described mathematically as:
$$precision = \frac{l}{c},\ \ \ \ recall = \frac{l}{t}$$ where \textit{l} denotes the \textit{the number of recommended items @k that are relevant}, {c} denotes the \textit{the number of recommended items @k} and \textit{t} denotes the \textit{total number of relevant items}. A detailed explanation with examples can be found on Medium.com \footnote{https://medium.com/@m\_n\_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54}.\\
\subsection{Distance using hyperlinks}
\label{secHyperlinkDistance}
A set with validated links must be available to generate an F1 score, as described in \ref{F1Score} and \ref{ranking}. This set will be considered the ground truth. The possible links in a set with $N$ documents will be $N x N$. If $N$ is significantly large, as is the case with the number of articles in Wikipedia, it is practically impossible to determine the correct set of links by human examination. On the other hand, an automatic algorithm must be very accurate because the number of relevant links must not be high. A high number of relevant links will result in a low F1 score because then the recall will become small (see formulas in  \ref{F1Score} and \ref{ranking}). It remains to be seen whether such an algorithm can be developed because limiting the number of possible links depends on the algorithm's perspective. The problems with the F1 score are described in section \ref{secIteration1}. 
 \\

Instead of a set containing validated links, it is helpful to assign a score to a link that represents the validity of the link. Using this score, the quality of the generated hyperlinks can be assessed. The documents' metadata will be used as input for calculating the score because this information is not available to the algorithm used in this thesis; it only uses plain text.\\
Except for the S2ORC dataset, all datasets mentioned in section \ref{secUsedDatasets} are composed of articles from Wikipedia. Wikipedia articles contain metadata in the form of hyperlinks that can be used to determine the relationship between two articles. An intuitive way to determine the extent to which the articles are related is to count the minimal number of hyperlinks that must be traversed from one article to another: the distance. \\
Wikipedia articles form a directed graph if each article is considered a node and each connection between articles is an edge connecting two nodes. This graph can calculate the distance mentioned above by calculating the shortest path between the nodes representing the articles of which the score has to be determined. An example of the article graph of the Dutch Wikipedia is given in figure \ref{imgArticleGraph}.\\

\begin{figure}[h]
\includegraphics[width=155mm]{images/ArticleGraphWikipedia.drawio.png}
\caption{Part of the directed article graph of the Dutch Wikipedia. It depicts the path from "Open Universiteit" to "Microsoft".}
\label{imgArticleGraph}
\end{figure}


To get good scores, the article graph and the document set must preferably be based on the same version of Wikipedia. For this thesis, a Python script was created that reads all hyperlinks from Wikipedia to generate the article graph. The script uses SQL files that are generated with each Wikipedia dump. First, for the names and ids of the articles, it uses `...-page.sql'. Second,  it uses the file '...pagelinks.sql', which contains all hyperlinks within these articles. The files contain a dump of all the corresponding tables in a MySql database. It takes a very long time to read all records in a database \footnote{https://stackoverflow.com/questions/43954631/issues-with-wikipedia-dump-table-pagelinks}. For that reason, the script interprets the statements in the dump file directly without first reading it into a database.\\

Graph databases can efficiently store graphs, as \citep{neo4j} states: "Graph database technology is an effective tool for modeling data when a focus on the relationship between entities is a driving force in the design of a data model." The paper uses NEO4J \footnote{https://neo4j.com} as an example graph database. This thesis explored using NEO4J to store the article graph for the English and Dutch Wikipedia. Unfortunately, it was not clear from their website what the license terms were. Because the downloaded version was limited to one database only and bulk import for large graphs could only be done once, NEO4J was not used. \\
The Functionality of Memgraph \footnote{https://memgraph.com} matches the Functionality of NEO4J and has a less restricted license model. Functionality to store the data in Memgraph was added to the Python Script. The problem with Memgraph was that it consumed much memory and took more than a day to load the data in the database, which meant that the turnaround time for changes to the script was long.\\

An alternative to using graph databases is using a Python script that stores the graph in memory. Unfortunately, the hardware used for this thesis has limited resources: a macOS machine with 16 GB of internal memory. Because the size of the graph is significant (more than 16.000.000 nodes for the English version of Wikipedia), the module must be memory efficient and fast. First Igraph \footnote{https://igraph.org} was considered because it was the easiest to use for development, but while loading the English version of Wikipedia, the machine ran out of memory.

NetworKit is a Python module in which "performance-aware algorithms are written in C++."\footnote{https://networkit.github.io/} Nodes in NetworKit are represented by integers, which means that article titles have to be translated to an integer and visa-versa. This translation is done by storing all Wikipedia article titles in an array and using the index in this array to represent the node. This method filters out all hyperlinks to articles that do not yet exist in Wikipedia. The translation and filtering were not enough to store the English version of Wikipedia in available memory. \\

Debugging the script revealed that much memory was consumed by articles that contained thousands of hyperlinks. An example of such an article is "DNA": it contains more than 3.000 hyperlinks. Articles with many hyperlinks make using the distance between two articles less valuable as a score for the validity of generated links. The shortest path between "Hippopotamus" (the animal) and "Frederick Griffith" (a British bacteriologist) is: $$ \text{Hippopotamus} \longrightarrow \text{DNA} \longrightarrow \text{Frederick\ Griffith}$$
However, if the DNA article is left out of the graph, there is no shortest path between "Hippopotamus" and "Frederick Griffith" which better matches reality. \\
When articles with more than 2.000 % todo: dit getal later nog nakijken, of refereren naar een ander deel van de thesis waar de grafieken staan
 links are left out of the graph, the English version of Wikipedia fits into available memory (hence the Dutch version, too). Saving the graph and intermediate data structures in a cache helped to increase performance dramatically.\\
 
 Intuitively the length of the shortest path in the article graph is a good score to determine whether a generated document link is valid. However, before using the score, the quality of the score has to be proven. The quality of the score can be assessed by calculating the score of validated links. GWikimatch, as described in section \ref{sectWikidata}, is a dataset containing links checked for validity by humans. In this thesis, a script was developed that calculates the shortest path length for each article combination in the set using the constructed article graph. \\

If the length of the shortest path is an indicator of the link's validity, a low value would indicate a valid link, and a high value would indicate an invalid link. In other words, the fewer times the user uses a hyperlink to travel from one article to another, the better the articles are related. To determine the correct value for the shortest path that separates valid from invalid links, the shortest path length and the link's validity are depicted in a bar diagram. Figure \ref{imgDistance2000} visualizes the relation between the shortest path length and the link's validity. From the figure, the conclusion can be drawn that, unfortunately, there is almost no relation between the length of the shortest path link's validity.\\

\begin{figure}[h]
\includegraphics[scale=0.8]{images/distance_2000.pdf}
\caption{The shortest paths between Wikipedia articles compared to the validity according to GWikimatch. Articles with more than 2000 hyperlinks have been removed. }
\label{imgDistance2000}
\end{figure}

\begin{figure}[h]
\includegraphics[scale=0.8]{images/distance_400.pdf}
\caption{The shortest paths between Wikipedia articles compared to the validity according to GWikimatch. Articles with more than 400 hyperlinks have been removed. }
\label{imgDistance400}
\end{figure}

A possible explanation could be that, as previously stated, articles containing many hyperlinks wrongfully minimize the length of the shortest path. The script was modified to remove articles from the graph with more than $n$ hyperlinks to rule out this possibility. Unfortunately, the value of $n$ had no significant impact on the relation between the length of the shortest path and the relation between the articles. Figure \ref{imgDistance400} shows the relation for the graph in which articles with more than 400 hyperlinks were removed. Appendix \ref{appendix:scorerelation} contains the figures for other values of $n$ that were examined. 

\subsection{Millne\&Witten}
\label{secHyperlinkDistance}
From the previous paragraph, it becomes clear that the length of the shortest path in the article graph is not a valid indicator for the relation between articles. Therefore, part of this thesis was to look for an existing score in literature representing the relatedness of two Wikipedia articles based on the hyperlink structure. \citet{Ponza2017} conducted a "study of all entity relatedness measures in recent literature based on Wikipedia as the knowledge graph." \\

In their study, \citet{Ponza2017} devised a system for calculating a relatedness measure between two Wikipedia articles. The study found the  CoSimRank algorithm for \citet{cosimrank} to be a good measure for calculating this. The disadvantage of this algorithm is that it does not perform well on large graphs like the Wikipedia article graph. In order to overcome the performance problem, a small subgraph is constructed consisting of articles close to the original articles. The CosSimRank algorithm uses this subgraph to perform its calculations. \\

The system devised by \citet{Ponza2017} is too complicated to be implemented for this thesis. It would have taken too much time and resources. For constructing the previously described subgraph, \citet{Ponza2017}, have analyzed several algorithms that use either the text of the article or its hyperlinks. The authors found that one of the best methods that use the hyperlink structure of Wikipedia is the Milne\&Witten score described in \citep{Witten2008} or DeepWalk, which is described in \citep{deepwalk}. The Milne\&Witten score can be calculated easily and quickly without taking too much time to implement. For these reasons, this score was chosen over the CoSimRank or Deepwalk algorithms to implement in this thesis. Figure \ref{formulaMilneWitten} contains the formula for calculating the Milne\&Witten score.\\

\begin{figure}[h]
$$1 -\frac{ log( max( |A|, | B| )-log(| A \cap B|))}{log(|W|)-log(min\{|A|, |B|\})}$$
\caption{Formula for the Milne\*Witten score, where a and b are the two articles of interest, A and B are the sets of all articles that link to a and b, respectively, and W is the entire Wikipedia. }
\label{formulaMilneWitten}
\end{figure}

The formula uses hyperlinks that point to an article as input. The script that creates the article graph, as described in section \ref{secHyperlinkDistance}, was expanded. Next to building an article graph, it creates a dictionary that uses the article index as the key and a list of indexes of articles that contain a hyperlink to this article as value. The Milne\&Witten value can be calculated efficiently using this data structure.\\

Analogous to the shortest path using the article graph, the Milne\&Witten value is depicted in a diagram with bar charts. The correct value for the Milne\&Witten score can be determined that separates the valid links from the invalid links by using this diagram. The diagram is shown in figure \ref{imgMilneWitten2000}.\\

\begin{figure}[h]
\includegraphics[scale=0.8]{images/milnewitten_2000.pdf}
\caption{The Milne\&Witten score of Wikipedia articles compared to the validity according to GWikimatch.}
\label{imgMilneWitten2000}
\end{figure}

When the Milne\&Witten score is a valid indicator for the validity of hyperlinks, there is a range of scores where valid links outnumber the invalid links significantly, and there is a range of scores for which the opposite is true. Unfortunately, the diagram shows that for every value of the Milne\&Witten score, the number of valid links is approximately equal to the number of invalid links. From the diagram, the conclusion can be drawn that more than solely using the Milne\&Witten score is needed to indicate whether a link is valid or invalid. 

\subsection{Using validation sets}
\label{secUsingValidationSets}
Neither the shortest path length nor the Milne\&Witten score is a good indicator for determining whether a link is valid, as described in the previous sections. To make maximal use of available resources for this thesis, a better approach would be to use data sets that can be used as gold-standard. One candidate is the GWikiMatch dataset described in section \ref{sectWikidata}.\\

To validate their research \citet{Ponza2017} have created a data set and made it publicly available for other researchers to validate their work. The dataset is called WiRe. The dataset contains 503 links from the English Wikipedia that have been assigned a score between 0 and 10. The construction of the dataset was done in three steps. First, the dataset of 100.000 entity pairs constructed from New York Times articles (\citep{Dunietz2014}) was used as a base. Second, the resulting dataset was filtered and balanced by several algorithms. Last, two human evaluators assigned every article pair of the resulting dataset a score between 0 and 10. When their scores did not coincide, a third human evaluator discussed the score with the original evaluators. The process resulted in the WiRe dataset. A thorough description of the process can be found in \citep{Ponza2017}.\\

Another dataset that \citet{Ponza2017}  have used for validation is the WikiSim dataset. It was created for validating the semantic relatedness obtained from Wikipedia links as described in \citep{Witten2008}, based on the WordSim 353 dataset (\citet{word353}), humans annotated it. The document pairs have been rated with a value between 0 and 1. Unfortunately, the details of the annotation process are not described in the paper.\\ %todo: ervoor zorgen dat dataset overal aan elkaar wordt geschreven (door het hele document)

Analogous to the GWikiMatch dataset, a script was created that translates the Wikipedia-ids to WikiData labels. This script enables the translation of the articles to Dutch and the reusing of already created software. The relationships are, as in the GWikiMatch dataset, also reversed. The WikiSim dataset's score, which has a value between 0 and 1, is translated to a value between 1 and 10, so it is compatible with the WiRe dataset. The resulting datasets and the cumulative number of documents contained within the dataset are depicted in figure \ref{imgWireDistr} and listed in table \ref{tabCutoff}. As can be seen from this table, the cutoff value is 6. Using this cutoff means that every value between 0 and 6 will be translated to a resulting value of 0 and 1 otherwise. The resulting values will be almost evenly distributed, which is what one would choose intuitively.

%todo: hier nog bijschrijven dat in de tabel de verschillende randwaarden staan tussen 0 en 1, wat als ik de grens bij 8 leg, wat bij 7 enz. Toch kiezen voor een goede link. Waarschijnlijk 7 of 8 

\afterpage{
	\begin{figure}[h]	
\includegraphics[scale=0.8]{images/WiReWikiSimDistribution.pdf}	
\caption{The distribution of the Wire and WikiSim data sets where the validity of the relation has a value between 0 and 10.}	
\label{imgWireDistr}	
\end{figure}	

	\begin{table}
	\centering
	\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l|l}
	        & 0  & 1  & 2  & 3   & 4   & 5   & 6   & 7   & 8   & 9   & 10   \\
	\hline
	\textbf{Wire}    & 26 & 63 & 94 & 110 & 141 & 178 & \textit{\textbf{246}} & 346 & 478 & 503 & 503  \\
	\hline
	\textbf{Wikisim} & 2  & 8  & 24 & 45  & 64  & 90  & \textit{\textbf{137}} & 192 & 255 & 267 & 268 \\
	\hline
	\end{tabular}
	    \caption{Cumulative number of pairs per dataset. The cutoff is shown in italic}.
	\label{tabCutoff}
	\end{table}
}
\pagebreak


