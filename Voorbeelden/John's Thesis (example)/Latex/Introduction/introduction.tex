\chapter{Introduction}

When searching for information, users often use search engines like Google\footnote{https://www.google.com} or Bing\footnote{https://www.bing.com}. These search engines have access to a wealth of publicly available information. To make finding the information even simpler and for processing and analyzing it faster, lately, Large Language Models like ChatGPT\footnote{https://www.chatgpt.com} and Google's Bard\footnote{https://bard.google.com} have been developed. The problem with the search engines and the LLMs is that they only have access to publicly available information. Additionally, the LLMs give answers instead of search results. This is very convenient, but the user cannot verify the answer because it contains no references to the sources the information comes from.\\

A source of information that is generally not publicly available is a knowledge base. A knowledge base is a specialized digital library containing documents related to a specific subject. To enable users to find the documents they need, the knowledge bases often contain a search engine that provides a quick and easy method of finding documents. However, users must know what they seek to use a search engine effectively. The search engine's result list only hints at the right documents. This does not account for serendipity, "the finding of unexpected information (relevant to the goal or not) while engaged in any information activity" \citep{andre2009discovery}, which is an important component of information retrieval \citep{foster2003serendipity}. Another drawback is that users are not able to browse through the documents other than via hyperlinks in the text.\\

A way to solve this in a knowledge base is to present links to documents with similar content \citep{makri2014making}. Often, these links are named "Recommended links," "Similar documents," "Read more..." and so on. Figure \ref{imgelsevier} contains an example of a document with links to similar content in a knowledge base. \\


\begin{figure}[h]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.197]{images/elsevier.png}
\caption{An arbitrary document from Elsevier Science Direct. The links in the yellow rectangle demonstrate the way semantic links can help users browse through the information.}
\label{imgelsevier}
\end{figure}

Links to documents or parts of documents with similar content are called semantic links. There are various ways to create these links. The most obvious way is to have editors create these links manually. This is, however, a very time and cost-intensive process. Therefore, ideally, these links should be created automatically. The techniques created for the Semantic Web \citep{berners2001semantic} provide a framework using an ontology with which semantic links can be created automatically. This is done using ontologies defining concepts and relations between concepts \citep{Erdmann:2000vn}. Semantic links can be generated by including these concepts in the documents. Subsequently, the relation between the concepts from the ontology can be translated into semantic links. Although links are created automatically, assigning concepts to a document and creating and maintaining the ontologies must be done manually. Semantic links can also be created without manually manipulating the content. This can be done using artificial intelligence (AI) models that "understand" the natural language. Modern versions of such models need many documents (more than 100K documents) to be trained and a lot of computer resources to work.\\

Because a knowledge base typically contains no more than 10K documents, and topics in a knowledge base come with their own jargon and specialized terms, or the documents are written in a language other than English, models that were trained on content from general texts or content from other knowledge bases ignore this special information and therefore cannot generate optimal semantic links. Ideally, a specialized model must be created for each knowledge base.

Another property of knowledge bases is the limited number of potential users. If a knowledge base is to be used commercially, the amount of computer resources that can be allocated is limited to constrain the operating costs. This creates a need for models that can be trained on fewer than 100K documents and are resource-efficient.\\

The research conducted for this thesis aims to overcome the previously mentioned obstacles: large training sets, using languages other than English, and limited resources. Software is created to use an enhanced version of the large-scale hierarchical alignment (LHA) algorithm \citep{nikolov2018large}. This software is named SADDLE (Section Alignment to Detect Document Links Efficiently). Where the LHA algorithm is designed for aligning sentences in documents, SADDLE finds similarities between sections and documents. Both algorithms are constructed to work with 10K documents or less and are scalable.\\

A key success factor for SADDLE is to use good training and validating sets. One option is to create the sets using ontologies that were produced for the Semantic Web and to combine these ontologies with articles from Wikipedia.\\

The above leads to the following main research question of this thesis: \textit{How can hierarchical decomposition into sections and encoding help find automated relations between documents or sections in document sets that contain less than 10K documents?}

To answer this question, the CRoss-Industry Standard Process for Data Mining (CRISP-DM) research method\footnote{https://www.datascience-pm.com} is used. Firstly, this means that at least one, preferably more representative training- and validation sets must be found or created. Secondly, the SADDLE algorithm is improved in several steps. Every step is validated to determine the quality of the improvement. A detailed description of the research method is given in section \ref{secResearchMethod}.  \\

This thesis is structured as follows. Chapter \ref{sectRelatedWork} overviews research on semantic relations between documents. The research question, the subquestions, the research method, and how the research is conducted and validated are outlined in section \ref{secResearch}. Chapter \ref{secRCI_II} elaborates on the datasets that are publicly available at this moment. An attempt was made to create a topic-based dataset (chapter \ref{chapTopicDatasets}). The results of experiments using the SADDLE algorithm can be found in chapter \ref{secResults}. The last chapter, chapter \ref{secConclusions}, contains the concluding remarks, identified limitations, and possible future work perspectives.

