\pagebreak
\chapter{Research background}
\label{sectBackground}

\section{F1 Score}
\label{F1Score}
Evaluation metrics quantify the quality of the semantic links generated by an algorithm. They can be used to compare the quality of various algorithms and help improve an algorithm while it is in development. The F1 metric \citep{forman2003} is an often-us evaluation metric in NLP research. It is a measure of an algorithmâ€™s accuracy on a dataset, represented by the formulas \ref{eqF1a}, \ref{eqF1b} and \ref{eqF1c}.

\begin{subequations}
\begin{equation}
\label{eqF1a}
F1 = \frac{ 2 \times recall \times precision}{recall + precision}\end{equation}
\begin{equation}
\label{eqF1b}
precision = \frac{|\{relevant\ documents\}\  \cap\  \{retrieved\ documents\}|}{|\{retrieved\ documents\}|} 
\end{equation}
\begin{equation}
\label{eqF1c}
recall = \frac{|\{relevant\ documents\}\  \cap\  \{retrieved\ documents\}|}{|\{relevant\ documents\}|}
\end{equation}
\end{subequations}\\


In a classification context, values for precision and recall can be calculated with the formulas \ref{eqF1d} and \ref{eqF1d}, where \textit{tp} denotes the \textit{true positives}, \textit{fp} the \textit{false positives} and.  \textit{fn} the \textit{false negatives}.\\

\begin{subequations}
\begin{equation}
\label{eqF1d}
precision = \frac{tp}{tp + fp}
\end{equation}
\begin{equation}
\label{eqF1e}
recall = \frac{tp}{tp + fn}
\end{equation}
\end{subequations}\\


\section{Ranking}
\label{ranking}
Semantic link generation can be considered a classification problem, meaning a valid result in the generated links and the validation data is considered a true positive. However, it is more specifically a ranking problem in which the most relevant semantic links must not only be retrieved but also placed at the top of the ranking. \cite{Rajaram2003} argue, "If the ranking problem is posed as a classification problem, then the inherent structure present in ranked data is not used of, and hence generalization ability of such classifiers is severely limited.".   The generation of semantic links can also be viewed as a ranking problem for search, with the document being the query and the semantic links representing the search results. \\

By using precision at K (P(K)), \citep{Agichtein2006} an F1 score can be calculated for this ranking problem. P(K) reports the fraction of documents ranked in the top K results labeled as relevant. The position of relevant documents within the top K is irrelevant. This can be described mathematically as:
$$precision = \frac{l}{c},\ \ \ \ recall = \frac{l}{t}$$ where \textit{l} denotes the \textit{the number of recommended items @k that are relevant}, {c} denotes the \textit{the number of recommended items @k} and \textit{t} denotes the \textit{total number of relevant items}. A detailed explanation with examples can be found on Medium.com\footnote{https://medium.com/@m\_n\_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54}.\\

\section{Wikidata \& DBPedia}
\label{sectWikidataDBPedia}

Wikidata \citep{vrandevcic2014Wikidata} and DBPedia \citep{dbpedia} are knowledge graphs linked to Wikipedia articles and can potentially be used for generating suitable datasets. The DBPedia knowledge graph is created by extracting structured content from Wikipedia information. On the other hand, the knowledge graph created by the Wikidata project serves as an information source for projects like Wikipedia. This means that DBPedia uses Wikipedia as a source, and Wikidata is a source for Wikipedia. This means that Wikidata is better suited for creating datasets because the text from Wikidata does not necessarily have to be contained in the final Wikipedia article. The Wikidata knowledge graph is less dependent on the information and structure of the Wikipedia article than DBPedia.\\

Wikidata is a knowledge graph that is multilingual by design and is published under legal terms that allow the broadest possible reuse \citep{vrandevcic2014Wikidata}. Information from Wikidata can be represented as semantic triples in the form of Resource Description Format (RDF) triples \citep{erxleben}. RDF triples are a standardized way to describe information. They are in the form \texttt{<subject, predicate, object>}. For example, the triple \texttt{ <dog, produced sound, bark>} indicates that a dog can make a barking sound. Because Wikidata is multilingual by design, IDs represent the elements of the triple that can be translated into a description in one of the many languages supported by Wikidata. The Wikidata RDF triple \texttt{<wd:Q144, wdt:P4733, wd:Q38681>} describes the previous example.\\ 


\section{Wikipedia subgraphs}
\label{secWikipediaSubgraphs}
\citet{Ponza2017} conducted a "study of all entity relatedness measures in recent literature based on Wikipedia as the knowledge graph." In their study, they devised a system for calculating a relatedness measure between two Wikipedia articles. The study found the  CoSimRank algorithm \citep{cosimrank} to be a good measure for calculating this. The disadvantage of this algorithm is that it does not perform well on large graphs like the Wikipedia article graph. To overcome the performance problem, a small subgraph is constructed consisting of articles close to the original articles. The CosSimRank algorithm uses this subgraph to perform its calculations. \\

The system devised by \citet{Ponza2017} is too complicated to be implemented for this thesis. It would have taken too much time and resources. For constructing the previously described subgraph, \citet{Ponza2017}, have analyzed several algorithms that use either the text of the article or its hyperlinks. The authors found that one of the best methods using the hyperlink structure of Wikipedia is the Milne\&Witten score described by \citet{Witten2008} or DeepWalk, described by \citet{deepwalk}. The Milne\&Witten score can be calculated easily and quickly without spending too much time. For these reasons, this score was chosen over the CoSimRank or Deepwalk algorithms to implement in this thesis. Formula \ref{formulaMilneWitten} contains the formula for calculating the Milne\&Witten score.\\

\begin{equation}
\captionsetup{justification=centering}
\label{formulaMilneWitten}
1 -\frac{ log( max( |A|, | B| )-log(| A \cap B|))}{log(|W|)-log(min\{|A|, |B|\})}
\end{equation}
\\ 
The formula \ref{formulaMilneWitten} calculates the Milne\&Witten score, where a and b are the two articles of interest, A, and B are the sets of all articles that link to a and b, respectively, and W is the entire Wikipedia.

\section{Training neural networks}
\label{training}
Neural networks need a dataset to be trained. To properly train a model, the dataset is split into three datasets: a training set, a validation set, and a test set (See figure ...). Firstly, the basic training of the parameters of a model is done using the training set. Secondly, the model is trained several times to determine the optimal hyperparameter configuration. The validation set is used to find the most optimal configuration by selecting the configuration with the lowest error on the validation set. Lastly, the test set is used to determine the model's performance. This set cannot be the same as the validation set because the validation set was used to determine the configuration of the model and, therefore, can be biased. In research, 60\% of the dataset is often used as a training set, 20\% as a validation set, and 20\% as a test set.\\

\begin{figure}[h]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.75]{images/Datasetsplit.drawio.pdf}
\caption{A dataset is split into separate sets for training, validation and test purposes.}
\label{imgKFold}\end{figure}


When the dataset size is relatively low, one wants to make optimal use of the dataset. By splitting it into three sets, as mentioned above, even fewer items are available for training. K-Fold Cross-Validation is an often-used method to use all values for training without "losing" values for validation. With this method, the dataset is split into K folds, where one is used for validation, and K-1 folds are used for training. In the next iteration, another fold is used for validation, and the remaining folds are used for training. This process is repeated K-times until all folds are used for validation. The final result of the training is the average of the accuracy and F1 value of all iterations. Using K-Fold Cross-Validation, the relatively small number of training examples available in Wikisim and WiRe is used efficiently. This advantage compensates for the increased computer resources needed to train the neural network. A graphical representation of K-Fold Cross-Validation is depicted in figure \ref{imgKFold}.  
\\

\begin{figure}[h]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.25]{images/kfold.png}
\caption{K-fold Cross-Validation with K=5. (The image was copied from \\https://towardsdatascience.com /cross-validation-k-fold-vs-monte-carlo-e54df2fc179b).}
\label{imgKFold}\end{figure}



