\pagebreak
\chapter{Research approach}
\label{secResearch}

Creating semantic relations between documents is beneficial for information retrieval and information recommendation. When used in a knowledge base, a data repository containing information about a specific topic, it can help users find the necessary information. Users can use the semantic relations to navigate through the knowledge base.\\

The available algorithms mostly cannot be used for knowledge bases. Firstly, although very large knowledge bases exist, they are often small in practice because they contain information about a specific topic on which not necessarily many documents are available. Most existing algorithms require many documents to be trained (more than 100K documents). Secondly, not all knowledge bases are in English, but they sometimes contain information in another natural language and, therefore, most of the time, have fewer documents. A model that can be trained on a smaller set of documents is better suited to these knowledge bases. Using a model trained on documents contained in the knowledge base is the most optimal because it is tuned to the specific topic of the knowledge base. \\

The large-scale hierarchical alignment (LHA) algorithm \citep{nikolov2018large}, as mentioned in section 2.3, is a suitable algorithm for creating semantic relations between knowledge base documents.
Although the LHA algorithm was created to extract parallel sentence pairs instead of similarities of sections and documents, it can be used as a base for a new yet-to-be-created algorithm (SADDLE) that extracts parallel section pairs. The information of the section pairs can be used for two purposes. First, it is used to create document relations. Second, when the relation is established, the section pairs can be shown to the end user to highlight the relation between the sections. The research for this thesis determines the optimal algorithm to create the embeddings of the sections and the embeddings of the document. \\

There is room for improvement of the LHA algorithm; insights from other NLP research can be applied. Firstly, it uses Sent2Vec and BERT to create sentence embeddings, and SentenceBERT or USE could replace this to improve sentence embeddings. Secondly, the position of the related sections in the document can reveal something about the importance of the relation. Intuitively, when the related sections are at the beginning of the document, the document relation becomes stronger (this principle is used by Page Rank \citep{florescu2017position}). A neural network could be used that determines the quality of the document relation relative to the document position of the matched sections.\\

Figure \ref{imgtotal} is an example of the output of this new improved algorithm. The sections from documents A, B, and C that have a corresponding color are related. Documents A and B have more corresponding sections than documents A and C. Therefore, Document A and Document B have a stronger semantic relationship than Document A and Document C.\\
 

\begin{figure}[h]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.45]{images/Total.drawio.png}
\caption{Sections I, II, and IV of document B are similar to sections IV, II, and  I of document B, respectively. Section VII of document C is similar to section II of document A and document B. That defines the similarity between document A, B and C.}
\label{imgtotal}
\end{figure}


\section{Research questions}
\label{secResearchQuestions}
With the research aims formulated in the previous section (section \ref{secResearch}) in mind, the following main research question has been formulated. Further, the main research question is structured in four smaller sub-questions as defined below: 

\textbf{Research question:}~\textit{How can hierarchical decomposition (into sections) and encoding improve on finding automated relations between documents in document sets compared to encoding all of the text of a document, where the document set contains less than 10K documents?}

\begin{itemize}
  \item[RQ1:] \textit{Which are known representative corpora for training and validation?}\\
A vital part of machine learning is a dataset on which the software can be trained and validated. For this research, not enough resources are available to create corpora manually. It is better to do research into existing corpora that are open-source and used for similar research. Preferably, the corpora are both in English and Dutch. Training SADDLE requires at least one set of documents, and multiple sets are preferable. 
  \item[RQ2:] \textit{What metrics can be used to assess the quality of the semantic links?}\\
Metrics are essential for machine learning because they provide a means to quantify the performance of machine learning models. The metrics are used for evaluation during and after training. The metrics are essential to compare different machine learning models. Because of the importance of the metrics, metrics used in existing research are explored.
  \item[RQ3:] \textit{What method can be used to create a dataset on a specific topic?}\\
    Generating datasets on a specific topic enables the SADDLE algorithm to be trained specifically for that topic. This can be achieved using software designed for this purpose. Wikipedia can be a useful source for these datasets, as it contains a wealth of information on a vast range of topics. By training SADDLE on a topic-specific dataset, it can become highly accurate and effective at generating semantic links to that topic. 
The quality of SADDLE is determined using a validation set. The documents in the validation set must not overlap with those in the training set. 
  \item[RQ4a:] \textit{What method gives the best result for computing the embedding of the sections and then creating document relations?}\\
  This question is the most challenging and influential factor in SADDLE's quality. The quality of the generated embeddings is essential for the quality of the generated semantic links. However, choosing the right method for combining the embeddings of the document's sections while comparing the documents is equally important for creating the correct semantic links.  
  \item[RQ4b:] \textit{To what extent can this method be extended to a different language than English (preferably Dutch)?}\\
  To answer this question, the training- and validation sets must be available in languages other than English, with which the dependence on a single language can be kept to a minimum. The reason that Dutch was chosen as a second language is that it is the native language of the researcher. Researching and validating documents that the researcher cannot understand is hard.
  \end{itemize}
The four subquestions are described further in section \ref{secCrispdm}.
\pagebreak

\section{Research method}
\label{secCrispdm}
This research is based on a data approach and modeling in a structured way, so a direct data science methodology is applicable.
A well-known research method for data mining is \textbf{CR}oss-\textbf{I}ndustry \textbf{S}tandard \textbf{P}rocess for \textbf{D}ata \textbf{M}ining (CRISP-DM). The method is based on best practices. It was created in 1999 and is still the most used framework for data science projects (or machine learning projects)\footnote{https://www.datascience-pm.com/crisp-dm-still-most-popular/}. 
The method consists of six phases that are performed iteratively. The phases are (as mentioned on the website of The Data Science Process Alliance\footnote{https://www.datascience-pm.com}): 
\begin{itemize}
  \item[1] Business understanding – What does the business/research need?
  \item[2] Data understanding – What data do we have/need? Is it clean?
  \item[3] Data preparation – How do we organize the data for modeling?
  \item[4] Modeling – What modeling techniques should we apply?
  \item[5] Evaluation – Which model best meets the business/research objectives?
  \item[6] Deployment – How do stakeholders access the results, or what do the results mean in the case of research?
\end{itemize}
These phases are visualized in figure \ref{imgcrisp}. The figure shows that phases 1 to 5 are done in iterations, and phase 6 is the final step. The business understanding phase is finished. The research aim is an outcome of the business understanding phase and is described in the previous sections. \\


\begin{figure}[h]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.7]{images/CRISP-DM_Process_Diagram.png}
\caption{CRISP-DM process diagram (the diagram was copied from https://en.wikipedia.org/wiki/Cross-industry\_standard\_process\_for\_data\_mining).}
\label{imgcrisp}
\end{figure}

RQ1 and RQ2 are answered during phase 2 of the CRISP-DM method. During this phase, a data corpus is selected and explored. To answer RQ1, a selection of Wikipedia lemmata together with a Wikidata, as proposed in \citep{ostendorff2020pairwise}, is initially examined as a dataset. When this is found not to be suitable, another dataset is used. A literature study is executed to find the metrics that can be used to validate the model. Finding a fitting dataset for evaluation completes the answer to RQ2 (see also section \ref{secMetrics}).\\

A preprocessing step is developed for the data preparation phase (phase 3). This filter receives the data selected in Phase 2 as input and produces data in a general format used in the modeling phase. By creating this extra filtering step, the model software is independent of the data structure to be processed. When a new dataset is chosen, only this filtering software has to be created or modified, and the model software can be left unchanged.\\

In the modeling phase (phase 4), a model is developed that creates embeddings for the sections of the documents with which the alignment of documents and sections can be calculated. Because finding the optimal structure of the model is the goal of the research, this model changes during the iterations of the CRISP-DM method. Configuring and validating a model can take many resources. Hence, a point of special interest is how to reuse data from a previous iteration, for example, by preserving data from a preprocessing computation to mitigate this problem.\\

Furthermore, after a model has been developed, it must be evaluated to assess its quality (phase 5). Using the metrics found by answering RQ2, the changes applied in every iteration can be evaluated. The metrics gathered using document embedding for creating document relations are used as a baseline. Together with the modeling phase, questions RQ3 and RQ4a can be answered during the evaluation phase.\\
The model is evaluated for non-English languages to answer RQ4b, which means a new dataset must be used. \\

Moreover, after all iterations have been performed, the metrics obtained during the previous phases and the iterations are used in the deployment phase.  In this phase, the final results of the research can be determined, and conclusions can be drawn.

 
\section{Algorithm implementation}
\label{secApproach}
\label{secResearchMethod}
As stated in the previous sections, this thesis aims to find semantic links in knowledge bases. A knowledge base contains articles, news items, and other long documents. These documents are called long-form documents, contrary to short-form documents like tweets, queries, answers, etc. Figure \ref{imgLongShort} shows applications of semantic text matching across the length spectrum of source and target texts. The upper-right area of the image depicts long-source to long-target comparisons, which have been less explored in the semantic text match setting than the other three segments of the image \citep{jiang2019semantic}. Unfortunately, this kind of text matching is necessary for finding semantic links in a knowledge base. According to \citet{Pang2021}, little research has been done on long-form-document to long-form-document comparison due to the lack of public datasets and efficient algorithms. \\

This thesis proposes an efficient algorithm for finding semantic links between documents and sections of documents. This algorithm is based on the LHA algorithm as described by \citet{nikolov2018large} and is called "Section Alignment Detecting Document Links Efficiently" (SADDLE). The first part of this section describes the LHA algorithm, and the second part describes the modifications made to the LHA algorithm to create the SADDLE algorithm.


\begin{figure}[h]
\centering
\captionsetup{justification=centering}

\includegraphics[scale=0.12]{images/SchemeLongShort.png}
\caption{Applications across different lengths of source and target documents in text retrieval. (edited from \citep{jiang2019semantic}) }
\label{imgLongShort}
\end{figure}

\subsection{The LHA algorithm}
\label{subsecLHAAlgortithm}

There are multiple reasons why the LHA algorithm was chosen over creating a new algorithm form scratch or choosing another algorithm as a base. Firstly, creating a new algorithm from scratch takes much more time than using a suitable algorithm as a base. Secondly,  the algorithm's properties, being "robust to noise, fast and memory efficient," are also required for SADDLE. Thirdly, because the algorithm performs the tasks in two phases,  and thus, the code of the algorithm consists of two parts, the code of the algorithm is easier to understand and modify. Fourthly, the LHA algorithm uses pre-trained data, which means that training the algorithm needs no extra training data and can be run on small datasets like knowledge bases. Fifthly, the source code of the LHA algorithm is publicly available, and the license permits altering the code. Lastly, the LHA algorithm enables choosing an existing base algorithm (Word2Vec or Sent2Vec, for example). Fortunately, the architecture of the algorithm enables adding a new algorithm without much effort.\\

\citet{nikolov2018large} "propose a simple unsupervised method for extracting pseudo-parallel monolingual sentence pairs from comparable corpora representative of two different text styles, such as news articles and scientific papers ." It uses pre-trained embeddings of documents and sentences and does not require a seed parallel corpus. The paper describes creating the LHA algorithm to extract pseudo-parallel sentence pairs from two raw monolingual corpora containing documents in two different writing styles (for example, simple Wikipedia versus standard Wikipedia.) \\

Given two datasets, a \textbf{source} dataset $S^a$,  consisting of $N_s$ documents (or articles) $S^a = \{s^a_1, \ldots, s^a_{N_s}\}$ and a and a \textbf{destination} dataset $D^a$,  consisting of $N_d$ articles $S^d = \{d^a_1, \ldots, d^a_{N_d}\}$ the approach described in the paper is hierarchal and divided into two phases. The first phase aligns the documents, and the second aligns the sentences. The phases are depicted in figure \ref{imgLHAphasesOriginal}.

\begin{figure}[h]
\centering
\captionsetup{justification=centering}

\includegraphics[scale=0.3]{images/lha_phases}
\caption{Phases of the LHA algorithm, first documents are aligned, next sentences of matching documents are aligned (copied from \citep{nikolov2018large}).}
\label{imgLHAphasesOriginal}
\end{figure}
\begin{itemize}
  \item{\textbf{Document alignment}}. This phase aims to select document pairs with high semantic similarity, the idea being that similar documents contain good pseudo-parallel sentence pairs. For each document in the source dataset, the LHA algorithm retrieves the $K$ nearest neighbors $\{d^a_{i_1}, \ldots, d^a_{i_K}\}$ from the destination dataset. Firstly the algorithm computes the document embedding $e_a()$ as $I_s = [e_a(s^a_1), \ldots, e_a(s^a_{N_s})]$ and $I_d = [e_a(d^a_1), \ldots, e_a(d^a_{N_d})]$. Based on this embedding, the nearest neighbors are determined by fast and efficient nearest neighbor search methods to find similar documents across $I_s$ and $I_d$. Additionally, the document pairs are filtered out, of which the similarity value is below a manually selected threshold. 
    \item{\textbf{Sentence alignment}}. After selecting document pairs with a high similarity $(s^a, d^a)$ containing a \textbf{source} $s^a = \{s^s_1, \ldots, s^s_{N_J}\}$ consisting of $N_J$ sentences and a \textbf{destination} $d^a = \{d^s_1, \ldots, s^s_{N_M}\}$ consisting of $N_M$ sentences, pseudo-parallel sentence pairs with a high similarity $(s^s_i, t^s_j)$ are computed.  The $K$ nearest neighbors are extracted using a similarity matrix $P$ to create the sentence pairs. $P$ contains the inter-sentence similarity for each sentence pair in $s_a$ and $d_a$. The nearest neighbors of $s^s_i$ are denoted as $NN(s^s_i) = \{d^s_{i_1}, \ldots,d^s_{i_K}\}$ and of $d^s_i$ as $NN(s^s_j) = \{d^s_{j_1}, \ldots,d^s_{j_K}\}$. A manually set threshold $\theta_s$ filters out the low-similar sentence pairs. After this step, all overlapping sentence sets are merged to produce pseudo-parallel sentence sets (see section 3.2 of the paper by \citet{nikolov2018large}).
\end{itemize}

    
\subsection{Implementation of the LHA algorithm for verification}
As mentioned in previous sections, the SADDLE algorithm is based on the LHA algorithm with minor and significant changes. This section describes the minor changes to the LHA algorithm and ends with an architecture in which the first phase of the LHA algorithm is broken up into individual modules. To be sure the LHA algorithm is implemented correctly, the results of the implementation of the first phase algorithm are compared to the results of the original paper. \\

First, the nearest neighbor search using approximation is deleted. The original LHA algorithm was intended for millions of documents, but this thesis focuses on knowledge bases with about 10k documents. Because the speed gained by using an approximate nearest neighbor search is unnecessary, leaving it out means that the complexity of the resulting algorithm is reduced without compromising the accuracy.

Second, the algorithms InferSent \citep{Infersent} and BERT are not being used because they are not suitable for document alignment (see table 1 of the paper by \citet{nikolov2018large}). For this thesis, only the Sent2Vec (\citep{pagliardini2017unsupervised}) and average word embeddings (Avg) are implemented.


The architecture of the first phase of the LHA algorithm in distinct modules is depicted in figure \ref{imgLHA_v1}. 

\begin{figure}[h]
\centering
\captionsetup{justification=centering}

\includegraphics[scale=0.36]{images/LHA_v1.drawio.png}
\caption{Architecture of phase 1 of the LHA algorithm}
\label{imgLHA_v1}
\end{figure}

\subsection{The adaption of the LHA algorithm for SADDLE}
\label{secSADDLE}
As stated previously, SADDLE is an efficient algorithm for finding semantic links between documents and sections of documents. The concept of the LHA model, where documents are filtered on similarity, and only these similar document pairs are being processed, is used by SADDLE. Because SADDLE aims to be an algorithm that can be used on small training sets, it cannot rely on complicated neural networks that must be trained. Like the LHA algorithm, SADDLE extensively uses pre-trained datasets to overcome this problem.\\
However, instead of the two phases used by LHA, SADDLE uses three phases:
\begin{itemize}
  \item{\textbf{Document alignment}}. This phase selects the document pairs with high semantic similarity and is identical to the first phase of LHA.
  \item{\textbf{Section alignment}}. Whereas the LHA algorithm aligns sentences, the SADDLE algorithm aligns document sections. To accomplish this, it first splits the documents into sections. Next, an embedding of the sections is calculated. The same algorithms are used for the initial embedding calculation as in the first phase, Sent2Vec, and the average word embedding. Thus, developing the software takes less time because no new algorithms have to be implemented. \\
The method for finding similar sections is analogous to the LHA algorithm finding similar sections. The last step of the LHA algorithm, where overlapping sentences are merged, is not applicable for sections and, therefore, not a part of SADDLE.
After selecting document pairs with a high similarity $(s^a, d^a)$ containing a \textbf{source} $s^a = \{s^s_1, \ldots, s^s_{N_J}\}$ consisting of $N_J$ sections and a \textbf{destination} $d^a = \{d^s_1, \ldots, s^s_{N_M}\}$ consisting of $N_M$ sections, pseudo-parallel section pairs with a high similarity $(s^s_i, t^s_j)$ are computed.  The $K$ nearest neighbors are extracted using a similarity matrix $P$ to create the section pairs. $P$ contains the inter-section similarity for each section pair in $s_a$ and $d_a$. The nearest neighbors of $s^s_i$ are denoted as $NN(s^s_i) = \{d^s_{i_1}, \ldots,d^s_{i_K}\}$ and of $d^s_i$ as $NN(s^s_j) = \{d^s_{j_1}, \ldots,d^s_{j_K}\}$. A manually set threshold $\theta_s$ filters out the low-similar section pairs. 
  \item{\textbf{Refining similarties}}. The third and final phase uses the information gathered during the alignment of the sections to determine whether document pairs are good matches. The embeddings are used to train a small neural network that calculates a score for the similarity of the document pair. Details of the implementation of this network are described in section \ref{phase3}.\\
\end{itemize}

All phases are depicted in figure \ref{imgsadllearch}. This figure shows the overall architecture of SADDLE and will be referenced throughout the thesis.\\

\begin{figure}[h]
\centering
\captionsetup{justification=centering}

\includegraphics[scale=0.36]{images/Saddle_phase_2.drawio.png}
\caption{Complete architecture of SADDLE}
\label{imgsadllearch}
\end{figure}



\subsection{Evaluation of SADDLE}
\label{secSADDLEVerification}

The SADDLE algorithm implementation is difficult to verify because no reference material could be found. The document alignment of LHA and SADDLE's first phase is identical, meaning that this phase's implementation needs no second verification.\\

The second phase will be loosely verified because of the absence of reference material to validate. Firstly, a graphical, interactive interface is created in which every document pair is shown and highlights the sections that have similarities in the other document. The corresponding sections in the other document are highlighted when such a section is clicked. Obvious errors in the implementation can be found by opening some document pairs using this graphical interface and checking whether the sections look similar. Figure \ref{imggrafischeinterface} depicts a screenshot of this interface. A second verification of the implementation of phase 2 is combined with the verification of phase 3.\\

The third phase can only be verified by running all phases of SADDLE sequentially. Using a validation set to determine the quality of the matched document pairs should be better for phases one to three than for phase 1 alone. When this is not the case, it must be that either using section
alignment to create semantic links does not work or phase two or three has not been implemented correctly.\\

\begin{figure}[h]
\centering
\captionsetup{justification=centering}

\includegraphics[scale=0.4]{images/voorbeeldgrafischeinterface.png}
\caption{Graphical interface. The grey sections have similar sections to those in the other document. The yellow section is clicked and shows which sections from the other document are related. The score of the relation is shown near the arrows.}
\label{imggrafischeinterface}
\end{figure}

\subsection{Validation of SADDLE}
This thesis aims to design an algorithm that improves the LHA algorithm for semantic relations between documents or sections of documents. Three steps must be taken to determine whether this aim has been reached. Firstly, a good validation set has to be found. The results can then be translated into concrete quantities using the right metrics. Finally, the values of the base LHA algorithm, thus computed, is compared to those of the improved algorithm.\\

Finding a good validation set takes much work. Therefore, this issue is treated in a separate subquestion of the research question: RQ1, "Which are known representative corpora for training and validation?" It would be an obvious option to use the same method for validation that \citet{nikolov2018large} use to validate the LHA algorithm. The authors use empirical evaluation in three different forms. Firstly, the algorithm's output is compared to a readily available dataset of sentence pairs. Secondly, the algorithm's output is fed into a translation algorithm, of which the results are then evaluated. Thirdly, a human evaluation is done on the generated sentence pairs.

Unfortunately, the same validation cannot be used for SADDLE. The main reason is that the LHA algorithm is designed to generate sentence pairs, and SADDLE is concerned with finding pairs of documents (by using document sections). Also, a human evaluation would not be feasible due to the limited resources allocated to this thesis project.\\

Ideally, a validation set with pairs of documents and sections written in English and Dutch can be found. When such a set cannot be found, a validation with English and Dutch texts is used. A synthetic set must be generated if such a set is unavailable. This set can be created by using the documents for Wikipedia together with the Wikidata metadata and the hyperlinks from Wikipedia to generate semantic relations. Separate software is created to generate the synthetic validation set.  \\

The source code of the developed software is made publicly available so it can be used in new research projects. This code is split into two repositories, namely Wikidata-corpora\footnote{https://github.com/johnstegink/Wikipedia-corpora} and SADDLE \footnote{https://github.com/johnstegink/SADDLE}.

\section{General development principles}
\label{secDevelopment}
\subsection{Programming environment}
\label{subsecPython}
The research for this thesis was conducted using Python\tablefootnote{https://www.Python.org} as a programming language and IntelliJ's Pycharm\footnote{https://www.jetbrains.com/pycharm/} as an integrated development environment (IDE). The operating system used for developing the software and running the tools was macOS, which is a UNIX-based operating system.\\

Several of the main advantages of Python made it the preferred programming language for this thesis. Firstly, Python is the most popular language for machine learning and academic research into natural language processing and machine learning. This popularity is hard to prove with absolute numbers because the popularity of languages changes constantly, and only a little research has been conducted into this. However, an indication of Python's popularity is that all of the research mentioned in section \ref{sectRelatedWork} was conducted using Python as a programming language. Secondly, because of the widespread use of Python, previously developed software can easily be integrated into tools for new research. Thirdly,  the Python ecosystem contains many valuable libraries specially developed for natural language processing, machine learning, and data preparation. Lastly, Python is open-source and platform-independent, allowing the code to be published and used on multiple operating systems.  \\

Like using any programming language, using Python has disadvantages. First, because Python is an interpreted language instead of a compiled language, it tends to be slow. This disadvantage is alleviated using the previously mentioned libraries, mainly written in C, which creates code with fast execution speed. Secondly,  Python checks variable types at runtime instead of compile time. When using runtime checking, development can be slow because when a type error occurs at the end of a script, this script has to be run multiple times to detect and solve the error. However, the IDE can detect most type errors while writing the script, so the number of potential runtime errors will decrease. Lastly, using Python requires extra effort from the researcher because he has to acquire the required Python skills to conduct the research for this paper.\\

\subsection{Developed tools}
\label{subsecPython}

The software developed for this thesis is divided into two projects: creating, processing, and analyzing the data (Corpus) and creating the semantic links (Model). Both projects contain the following types of scripts:
\begin{itemize}
  \item Tools: Python scripts that can be run on the command line and perform a specific task, i.e., it takes one or more input files and creates one or more output files.
  \item  Library scripts: contain functionality used in one or more tools or isolated functionality. Library scripts are written in Python and mostly have object-oriented classes that provide the interface to the functionality.
  \item Shell scripts: scripts written for the ZSH\footnote{https://www.zsh.org} UNIX shell. The scripts call multiple tools so they can be run efficiently without the need to supply the individual command line options.
\end{itemize}

The architectural design is depicted in figure \ref{imgtotalarch}. For reasons of clarity, this design does not contain the module scripts.
\iffalse 
The Python scripts with a short description of their function are listed in table \ref{tabCorpusScripts} and table \ref{tabModelScripts}. \\

\begin{table}[!ht]
    \centering
    \captionsetup{justification=centering}

    \begin{tabular}{l|l|p{9.1cm}}
    \hline
        \textbf{Script} & \textbf{Type} & \textbf{Description} \\ \hline
        GWikiMatch.py & Library  & Class to process GWikiMatch files obtained from Google Research\tablefootnote{https://github.com/google-research/google-research/tree/master/gwikimatch} \\ \hline
        GWikiMatchCorpus.py & Library  & Script to create a corpus based on GWikiMatch \\ \hline
        Links.py & Library  & Class create links between files \\ \hline
        S2ORC.py & Library  & Class to read S2ORC Corpus \\ \hline
        S2ORCCorpus.py & Library  & Script to create a corpus based on S2ORC \\ \hline
        Sections.py & Library  & Class to split Wikipedia article into sections \\ \hline
        SimpleWikiCorpus.py & Library  & Script to create a corpus based on a selection from the simple Wikipedia combined with one of the normal Wikipedia \\ \hline
        WikidataCorpus.py & Library  & Script to create a corpus based on Wikipedia \\ \hline
        WikidataSparql.py & Library  & Class to query data from Wikidata \\ \hline
        Wikidata.py & Library  & Class to read articles with the given subject from \begin{figure}[hbt]
  \includegraphics{}
  \caption{Wikidata}
\end{figure}
 \\ \hline
        functions.py & Library  & Generic functions \\ \hline
        plotWiRe.py & Library  & Plots the distribution in a Wikisim or WiRe file \\ \hline
        WiRe2gwikimatch.py & Library  & Script to convert a Wikisim or WiRe CSV to a format that is compatible with GWikiMatch \\ 
    \end{tabular}
    \caption{Table containing a short description of the Python scripts of the Corpus project.}
	\label{tabCorpusScripts}
\end{table}
\begin{table}[!ht]
    \centering
    \captionsetup{justification=centering}

    \begin{tabular}{l|l|p{9.1cm}}
        \textbf{Script} & \textbf{Type} & \textbf{Description} \\ \hline    
        AnalyseWikiGraph.py & Tool  & Creates a graph of the Milne\&Witten scores \\ \hline
        corpus2html.py & Tool  & Creates an HTML version of the corpus that can be navigated by using the relations within the corpus \\ \hline
        createWikiGraph.py & Tool  & Create an article graph from the Wikipedia dumps with all links as vertices \\ \hline
        create\_lha\_files.py & Tool  & Create files that can be used with the original LHA tools\tablefootnote{https://github.com/ninikolov/lha}. The input files are in the format expected by the original LHA algorithm \\ \hline
        createrelations.py & Tool  & Create document relations based on the document vectors that were created with "createvectors.py" (Step 2) \\ \hline
        createrelationsfrom2.py & Tool  & Create document relations based on the document vectors created with "createvectors.py." The two corpora being compared (i.e., Simple Wikipedia and English Wikipedia) for validating the LHA algorithm \\ \hline
        createvectors.py & Tool  & Create document embeddings from the corpus. This can be done either by Word2Vec or Sent2Vec (Step 1) \\ \hline
        evaluate.py & Tool  & The tool evaluates the score and puts the metrics in a tab-separated file. This file is used by plot.py to plot a diagram. \\ \hline
        functions.py & Library  & Generic functions \\ \hline
        plot.py & Tool  & Plots the score as determined by evaluate.py \\ \hline
        DistanceIndex.py & Library  & Class that provides for methods to calculate the distance between vectors, optionally using an Approximate Nearest Neighbor index like ANNOY\tablefootnote{https://github.com/spotify/ANNOY} \\ \hline
        DocumentRelation.py & Library  & Class that represents a relation between two documents \\ \hline
        DocumentRelations.py & Library  & Class to group and query relations between documents \\ \hline
        DocumentVector.py & Library  & Class that represents a document vector \\ \hline
        DocumentVectors.py & Library  & Class to group and query document vectors \\ \hline
        AvgWord2VecEncoder.py & Library  & Encodes a document using the average of the word vectors \\ \hline
        Sent2VecEncoder.py & Library  & Encodes a document using the Sent2Vect model \\ \hline
        documentencoder.py & Library  & Base class for the encoders to provide for a uniform interface \\ \hline
        WikiGraph.py & Library  & Class that creates and uses a graph of Wikipedia links \\ 
    \end{tabular}
    \caption{Table containing a short description of the Python scripts of the Model project.}
	\label{tabModelScripts}
\end{table}
\fi

\begin{figure}[h]
\centering
\captionsetup{justification=centering}

\includegraphics[scale=0.75]{images/Architectuur.drawio.pdf}
\caption{Architectural design of the developed tools. The tools are split into two projects: the Model project and the Corpus project.}
\label{imgtotalarch}
\end{figure}

The tools in the corpus project use a specific dataset as input and convert it to a corpus. A corpus contains data in a generic format equal to all datasets. By using this generic format, the data preparation phase from the CRISP-DM project is independent of the CRIPS-DM modeling phase. Once the corpus is generated, the modeling phase can be run multiple times on the same data. The data preparation phase is described in chapter \ref{secRCIII} and the modeling phase in chapter \ref{secRCI_II}.




\pagebreak