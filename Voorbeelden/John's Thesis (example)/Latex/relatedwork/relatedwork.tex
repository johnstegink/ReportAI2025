\pagebreak
\chapter{Related work \& Research background}
\label{sectRelatedWork}
\label{sectBackground}


In the first sections of this chapter, an overview of the research conducted into semantic relations between documents will be given.  Natural Language Processing (NLP) is the area of Artificial Intelligence (AI) research that focuses on understanding, reading, and responding to human language. An introduction to NLP and several concepts used in this thesis is given in \citep{rao2019natural} and \citep{Goodfellow-et-al-2016}. This section discusses the context/background of this research and related studies. 

The last sections contain a theoretical background that was used during the research. These sections are meant as an introduction to the theory and help understand the research for this thesis.\\

\section{Text embeddings}

One of the subfields of NLP is to create embeddings of a document or part of a document. An embedding is a numerical representation of a text that contains the essence of the text and is represented as vectors with real numbers. Text embeddings can be used for a wide range of applications.

 Using the embedding vectors, the similarity between the texts can be calculated; a method often used to accomplish this is cosine-similarity, which has the feature that the larger the cosine-similarity is, the more similar the texts are. An example of vector embeddings of documents $A$, $B$, and $C$ is given in figure \ref{imgcossim}. For this example, the vectors are simplified to two dimensions to be shown in an image. The angle $\alpha$ is smaller than angle $\beta$, i.e. $\cos(\alpha)$ is larger than $\cos(\beta)$.  Because the embedding is a representation, the smaller the cosine between the embedding vectors, the more similar the documents are. In the example, documents $A$ and $C$ are more related than $A$ and $B$ or $B$ and $C$.

\begin{figure}[h]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.3]{images/cosine.png}
\caption{A two dimensional example of cosine-similartiy.\\Vector A en C are more similar than A and B or C and B.}
\label{imgcossim}
\end{figure}



Text embeddings can be created using statistical methods, of which Term Frequency, Inverted Document Frequency (TF-IDF) \citep{robertson2004tdidf} is one of the most well-known methods. The intuition behind TF-IDF is that a word is a significant term for the text when that term is frequently used in the text, but its average usage in other documents is much lower. This way, a text can be represented as a vector containing this ratio (document frequency and total frequency) for every term.\\

Another method for creating embeddings of texts is to sum up, or average, the embeddings of all the individual words in a document. Embeddings of words with similar meanings have similar vectors, and math operations can be performed with these vectors to a certain extent, for example, $King - Man + Woman = Queen$. Word embeddings are often pre-calculated on a large corpus of documents. A word embedding is determined by analyzing which words frequently co-occur with others; examples of algorithms that create word embeddings are Word2Vec \citep{mikolov2013distributed} and Global Vectors for Word Representation (GloVe) \citep{glove}.  \\

Semantic hashing can efficiently find related documents in a production environment \citep{semantichashing}. Semantic hashing means a deep graphical model learns word-count vectors from a large document set. The values of the deepest layer are used to represent the documents. When the deepest layer uses a small number of variables, this is mapped to a number using the values as bits. When documents are semantically similar, the difference between the numbers is small, and this property can be used to find related documents quickly. The binary codes are compared simply when the deepest layer uses more variables.\\
  
Another way to find relations between documents in a production environment with many variables is by using a nearest-neighbor search algorithm instead of finding the nearest neighbors of a document by comparing the relative distance to all other documents. As defined by WikiPedia\footnote{https://en.wikipedia.org/wiki/Nearest\_neighbor\_search}: "Nearest neighbor search (NNS), as a form of proximity search, is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in a dissimilarity function: the less similar the objects, the larger the function values." A prevailing and fast implementation of a nearest neighbor search algorithm is ANNOY (Approximate Nearest Neighbors Oh Yeah)\footnote{https://github.com/spotify/ANNOY}

\section{Transformers}
The problem with using word embeddings is that the context of the sentence in which the word occurs is ignored. Considering the sentences "The man was accused of robbing a \textit{bank}." and "The man went fishing by the \textit{bank} of the river." the word \textit{bank} has a different meaning. The word's meaning is determined by its context, i.e., the other words and the place in the sentence. \\

Neural networks are an excellent way to determine patterns in data. The neural network architecture is inspired by the neurons in the brains of humans. They contain an input layer, one or more hidden layers, and an output layer. The nodes connect and have an associated weight and threshold. The neural network calculates the values of these weights and thresholds during the training phase. Detailed information about neural networks can be found in literature, for example, \citet{rao2019natural}. For an explanation of training neural networks, see section \ref{secTrainingNeuralNetworks}.\\

These neural networks can help to solve the problem of finding the correct meaning of a word within its context. Examples of these neural networks are sequence-to-sequence (S2S) models, a special case of a general family of models called encoder–decoder models ~\citep{boekh8}. An encoder–decoder model takes a text sequence (often a sentence) and encodes this text as a numeric (vector) representation. This representation is fed into a decoder that decodes the numeric representation as a new text similar to the original text (as depicted in figure \ref{imgs2s}). 

\begin{figure}[h]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.7]{images/s2s.png}
\caption{Sequence to sequence model. The original text is encoded first and then decoded into a new text. The model learns by comparing the original text with the new text.}
\label{imgs2s}
\end{figure}

This model is often used for the translation of texts from one language to another (for example, English to Dutch) or from one writing style to another.\\

  As building blocks of these encoder-decoder models, different kinds of recurrent neural networks (RNN) \citep{rnn} are being used. An RNN is a neural network that has an internal state (memory) and can process input sequences. Examples of RNNs that are often used for sequence-to-sequence networks are: Long Short Term Memory recurrent neural networks (LSTM) \citep{lstm} and gated recurrent units (GRU) \citep{gru}.\\


A new type of encoder-decoder model is the transformer model \citep{vaswani2017attention}, which performs better than encoder-decoder models based on RNNs. A particular mechanism for the transformer model is depicted in Figure \ref{imgtransformer}. Besides the description by \citep{vaswani2017attention} a good explanation of the model is given in “The Annotated Transformer.”\footnote{https://nlp.seas.harvard.edu/2018/04/03/attention.html}. \\

A transformer is superior in quality compared to sequence-to-sequence models based on RNNs. A transformer can be computed parallel, whereas an RNN can only be computed sequentially. Both the encoder and the decoder of the transformer model have a somewhat different yet similar structure centered around the use of (self-)attention. They each consist of six repeated and identical layers, which differ slightly between the encoder and decoder due to the addition of the output in the decoder. Importantly, the (self-)attention mechanism in these layers is done for all combinations of words/elements in the processed sequences.  This causes the computational complexity of these layers and, hence, the whole network to become 
quadratic in the sequence length. In contrast, the computational complexity of RNNs is linear in the sequence length. That means a transformer uses much more computation, but if the sequence length is limited and parallelization is used, it is less of a problem. The pairwise comparison ensures that every word is compared, irrespective of the distance between the words in the pair.  The RNN compares the words indirectly by using the memory state, which has a limited capacity and can leak information. This is why transformers can exploit the full sequence context much better than an RNN.

\begin{figure}[h]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.2]{images/transformer_model.png}
\caption{The architecture of the transformer model, a very popular sequence to sequence model.}
\label{imgtransformer}
\end{figure}


One of the state-of-the-art techniques, based on the transformer model, is BERT: Bidirectional Encoder Representations from Transformers \citep{devlin2018bert} and the improved version of BERT, the Robustly Optimized BERT Pre-training Approach (ROBERTA) \citep{Liu2019RoBERTaAR}. These models create word embeddings based on the word's context within the sentence and can learn the relation of two consecutive sentences by Next Sentence Prediction (NSP). The models can be used as a filtering network, which means they can be used as a building block for new models. Pre-trained models of BERT and ROBERTA are available for many languages. Bertje \citep{devries2019bertje}, a BERT model, and RobBERT \citep{delobelle2020robbert}, a RoBERTa model, are examples of pre-trained models that are available for the Dutch language for example.\\


Next to word embeddings and relations between consecutive sentences, the BERT model has an output that can embed a single sentence. The quality of this sentence embedding could be better; an average of GloVe \citep{glove} word embeddings are often better \citep{reimers2019sentence}. To find similar sentences in a collection of 10K documents using a pairwise comparison with BERT would take up to $\frac{1}{2}.n.(n-1)$ computations (this would take 49~ 995~000 inference computations), requiring up to 65 hours of computation time). \\

The Universal Sentence Encoder (USE) is an embedding method \citep{cer2018universal} to calculate embeddings of texts of arbitrary size, although it is most suitable for short texts, like sentences.  USE is a model toolkit containing two algorithms: a deep averaging network (\textsc{DAN}) \citep{DAN} and an algorithm that uses the transformer architecture. The transformer-based sentence encoder performs as well or better than the DAN encoder. The DAN-based sentence encoder generally gives better results.\\

 The algorithm Sentence BERT \citep{reimers2019sentence} uses a siamese network \citep{jiang2019semantic} containing BERT and RoBERTa models to create sentence embeddings. The embeddings of 10K documents can be computed in approximately 5 seconds, and by using cosine-similarity, the comparison would take about 0.01 seconds. These embeddings outperform other state-of-the-art sentence embedding methods like InferSent \citep{conneau2017supervised} and Universal Sentence Encoder \citep{cer2018universal}, by 2.1 and 2.6 points, respectively, on SentEval \citep{conneau2018senteval}.
 
\section{Aligning}
\label{sectAligning}

Text aligning is used for NLP tasks like generating sentence pairs for model training, detecting plagiarism, finding comparable documents, and making citation recommendations. Aligning texts makes it possible to find similar parts in different documents. Such text parts can be corresponding sentences, paragraphs, or sections. The encoding process is one of the critical components of an NLP system to align texts \citep{zhou2020multilevel}. 

Hierarchically structured document encoders, of which Hierarchical Attention Networks (HAN) \citep{yang2016hierarchical} are often used implementations and are neural networks that combine Word2Vec \citep{mikolov2013distributed} vectors into sentence embeddings. These embeddings, in turn, are combined to be used as a document embedding. \citet{jiang2019semantic} use a HAN in a siamese network \citep{mueller2016siamese} to learn whether there is a relationship between two documents.\\


Because the system as described by \citet{jiang2019semantic} ignores structural correspondence between parts, \citet{zhou2020multilevel} propose a new approach: "Multilevel Text Alignment with Cross-Document Attention." The authors propose a deep neural network that can predict relationships across different document levels. These levels are document-to-document and sentence-to-sentence.

\begin{figure}[h]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.22]{images/mta}\caption{Architecture of the multilevel Text Alignment with Cross-Document Attention model.}
\label{imgmta}
\end{figure}

The structure of the model is shown in Figure \ref{imgmta}, where $x_i$ represents a word, $s_j$ represents a sentence, and $d_l$ represents a document. The contextualizer does the aggregation of the words into a sentence. This is a sub-network that takes the importance of the words (attention) into account. The aggregation of the sentences into the document is done similarly. The blue lines in the figure denote the cross-document attention. This can either be \textsc{Shallow} (without the dashed lines) or \textsc{Deep} (including the dashed lines).

The resulting model is a very deep neural network, especially the \textsc{Deep} version of the model. This means that it takes a lot of time and a lot of documents to be trained properly. \citet{zhou2020multilevel} conducted several experiments wherein the training took up to 48 hours for the most complicated model. The authors found that document-to-document relations are not optimal: "which could be attributed to the small size of the plagiarism dataset" when used with a dataset containing 23K documents. Other experiments that were conducted do not have this restriction. The datasets for these experiments consist of more than 130K documents. \\

\citet{nikolov2018large} describe an algorithm to create sentence alignments, called the "Large-scale hierarchical alignment (LHA)" algorithm. First, their algorithm computes the embeddings of all documents and finds similar ones using the nearest neighbors. They use several methods for computing the document embedding and compare the performance of the methods in their paper. Second, the nearest neighbors of the sentences of the previously found document pairs 
 are computed and used for the sentence alignment. The authors find \textit{Sent2Vec} \citep{pagliardini2017unsupervised} to be the fastest to compute and perform well on document embedding and sentence alignment. The authors state that "LHA is robust to noise, fast and memory efficient, enabling its application to datasets on the order of hundreds of millions of sentences." 

One of the reasons that the algorithm is fast to compute is that it divides the computation of sentence embeddings into two hierarchical steps. First, the algorithm computes document embeddings. This can be done fast but with less precision. The precision is unimportant because it is used for filtering the documents to be compared. The filtering, however, dramatically reduces the number of sentences to be compared (figure \ref{imgQP}), which must be precise and is thus less fast. Second, the algorithm aligns the sentences of the filtered documents.\\

\begin{figure}[hbt]
\centering
\captionsetup{justification=centering}
  \includegraphics[scale=0.6]{images/Quality_vs_performance.png}
  \caption{Quality vs Performance}
  \label{imgQP}
\end{figure}


Table \ref{tabAlgorithms} gives an overview of the algorithms described in the previous sections. The first column contains the name of the algorithm in the family of algorithms. The second column includes the year it was described for the first time. The third column contains "Yes" when the algorithm is based on a neural network and is used as such\footnote{The algorithm used pre-trained models that can be neural networks, but the algorithm itself is not a neural network}. The last column is "Yes" when pre-trained models are available for this algorithm.


\begin{table}[h!tbp]
    \centering
    \captionsetup{justification=centering}
    \begin{tabular}{p{7.5cm}|l|l|l}
    	\textbf{Algorithm} & \textbf{Year} & \textbf{Neural} & \textbf{Pretrained} \\ 
    	& & \textbf{network}\\
		\hline
        TF-IDF & 2004  & No & No   \\
		\hline
        Word embeddings & &  \\
		\hline
        \hspace{3mm}Word2Vec & 2013 & Yes & Yes \\ 
		\hline
        \hspace{3mm}GloVe & 2014 & Yes & Yes \\ 
		\hline
        Encoder-decoder & 1987 & \\
		\hline
        \hspace{3mm}LSTM & 1997 & Yes & No   \\ 
		\hline
        \hspace{3mm}GRU & 2014 & Yes & No \\
		\hline
        Transformers & 2017 & & \\
		\hline
        \hspace{3mm}BERT & 2019 & Yes & Yes  \\
		\hline
        \hspace{3mm}RoBERTa & 2019 & Yes & Yes  \\ 
		\hline
        \hspace{3mm}Sentence BERT & 2019 & Yes & Yes   \\ 
		\hline
        Deep Averaging Network (DAN) & 2016 & \\ 
		\hline
        \hspace{3mm}Universal Sentence Encoder & 2018 & No &  Yes \\
        Hierarchical Attention Networks (HAN) & 2016 & \\ 
		\hline
        \hspace{3mm}Multilevel Text Alignment with & 2019 & Yes &  No \\
        \hspace{3mm}Cross-Document Attention & 2020 & & \\ 
		\hline
        \hspace{3mm}Large-scale hierarchical alignment (LHA) & 2018 & No & No  \\ 
		\hline
    \end{tabular}
    \caption{An overview of the discussed algorithms used for embeddings and alignment.}
  	\label{tabAlgorithms}
\end{table}




\section{Text corpora}

A large number of systematically collected structured texts is called a text corpus. In NLP, corpora are used for training an algorithm, validating an algorithm, and for benchmarking. Many corpora exist, most in English, but corpora in other languages are also available. Common Crawl\footnote{http://commoncrawl.org}, for example, contains texts in over 40 languages. To be useful as a corpus for algorithms that determine semantic relations between texts, the corpus has to contain annotations about relations between the texts. Most of the existing corpora do not provide such information.\\

A source for texts to be used in NLP is scientific (academic) papers, but these come with limitations. First, scientific papers are always available in the PDF format. A PDF is a graphical format; extracting text from the PDF is no trivial feat. \citet{kdir20} state that "... tools for extracting text from PDF documents would often mix body and non-body texts." The authors created a tool that recreates the text as closely as possible but does not preserve the document structure. A way to circumvent the problem is to use a LaTeX source, but not all papers are written with LaTeX software, and when they are, the sources are not always available. The second limitation is that scientific papers are not always free. Third, scientific papers are mostly written in English. On the positive side, because scientific papers contain references to other scientific papers, a semantic relation between papers can be determined using citation- or hyperlink-based approaches.\\

Another source of text to be used for NLP corpora is Wikipedia. It is freely available in many different languages\footnote{https://en.wikipedia.org/wiki/List\_of\_Wikipedias\#Details\_table}. Dumps containing all articles written in a particular language can be downloaded for free. Hyperlinks can determine the relation between the articles and other articles in the text of an article.

Instead of links between articles, ontologies containing semantic relations between concepts from Wikipedia can be used for article relations. For example, DBPedia \citep{BIZER2009154} and Wikidata \citep{vrandevcic2014Wikidata} are large ontologies available for free. \citet{ostendorff2020pairwise} use the Wikidata ontology because it is "an open knowledge graph in which nodes represent items (e.g., Wikipedia articles) and edges represent properties of these items (e.g., a relation that connects two different articles)." The authors only use a set of properties as a relation and conclude that Wikipedia, in combination with an ontology, is suitable to be used as a corpus for algorithms that determine semantic relations between texts.\\

\section{F1 Score}
\label{F1Score}
Evaluation metrics quantify the quality of the semantic links generated by an algorithm. They can be used to compare the quality of various algorithms and help improve an algorithm while it is in development. The F1 metric \citep{forman2003} is an often-us evaluation metric in NLP research. It is a measure of an algorithm’s accuracy on a dataset, represented by the formulas \ref{eqF1a}, \ref{eqF1b} and \ref{eqF1c}.

\begin{subequations}
\begin{equation}
\label{eqF1a}
F1 = \frac{ 2 \times recall \times precision}{recall + precision}\end{equation}
\begin{equation}
\label{eqF1b}
precision = \frac{|\{relevant\ documents\}\  \cap\  \{retrieved\ documents\}|}{|\{retrieved\ documents\}|} 
\end{equation}
\begin{equation}
\label{eqF1c}
recall = \frac{|\{relevant\ documents\}\  \cap\  \{retrieved\ documents\}|}{|\{relevant\ documents\}|}
\end{equation}
\end{subequations}\\


In a classification context, values for precision and recall can be calculated with the formulas \ref{eqF1d} and \ref{eqF1d}, where \textit{tp} denotes the \textit{true positives}, \textit{fp} the \textit{false positives} and.  \textit{fn} the \textit{false negatives}.\\

\begin{subequations}
\begin{equation}
\label{eqF1d}
precision = \frac{tp}{tp + fp}
\end{equation}
\begin{equation}
\label{eqF1e}
recall = \frac{tp}{tp + fn}
\end{equation}
\end{subequations}\\


\section{Ranking}
\label{ranking}
Semantic link generation can be considered a classification problem, meaning a valid result in the generated links and the validation data is considered a true positive. However, it is more specifically a ranking problem in which the most relevant semantic links must not only be retrieved but also placed at the top of the ranking. \cite{Rajaram2003} argue, "If the ranking problem is posed as a classification problem, then the inherent structure present in ranked data is not used of, and hence generalization ability of such classifiers is severely limited.".   The generation of semantic links can also be viewed as a ranking problem for search, with the document being the query and the semantic links representing the search results. \\

By using precision at K (P(K)), \citep{Agichtein2006} an F1 score can be calculated for this ranking problem. P(K) reports the fraction of documents ranked in the top K results labeled as relevant. The position of relevant documents within the top K is irrelevant. This can be described mathematically as:
$$precision = \frac{l}{c},\ \ \ \ recall = \frac{l}{t}$$ where \textit{l} denotes the \textit{the number of recommended items of the top K results that are relevant}, {c} denotes the \textit{the number of recommended items of the top K results} and \textit{t} denotes the \textit{total number of relevant items}. A detailed explanation with examples can be found on Medium.com\footnote{https://medium.com/@m\_n\_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54}.\\

The Spearman correlation coefficient\footnote{https://en.wikipedia.org/wiki/Spearman\%27s\_rank\_correlation\_coefficient}
 and Kendall correlation coefficient\footnote{https://en.wikipedia.org/wiki/Kendall\_rank\_correlation\_coefficient} are other often used metrics for calculating whether a list of items, in this case, semantic links, have the correct order of significance. The ranking is more important than the absolute value of an item in case of finding semantic links. The Spearman correlation and the Kendall coefficient are between -1 and 1, where a higher value denotes a higher correlation. These are statistical coëfficents of which the theory is out of the scope of this thesis. \citet{APCCoefficient} propose a new correlation coefficient, the AP Correlation Coefficient (APC). The APC pays more attention to the higher-ranked items than the lower-ranked items. Both the Spearman and Kendall correlation treat these equally.\\

To use ranking values as a metric, it is necessary to have a list with the desired ranking for each document pair. This is often not easy to realize, and therefore, the ranking matric is not often used.


\section{Wikidata \& DBPedia}
\label{sectWikidataDBPedia}

Wikidata \citep{vrandevcic2014Wikidata} and DBPedia \citep{dbpedia} are knowledge graphs linked to Wikipedia articles and can potentially be used for generating suitable datasets. The DBPedia knowledge graph is created by extracting structured content from Wikipedia information. On the other hand, the knowledge graph created by the Wikidata project serves as an information source for projects like Wikipedia. Hence, DBPedia uses Wikipedia as a source of information, while Wikidata serves as a source for Wikipedia. This makes Wikidata more appropriate for creating datasets since the text from Wikidata doesn't always need to be included in the final Wikipedia article. The Wikidata knowledge graph is less dependent on the information and structure of the Wikipedia article than DBPedia.\\

Wikidata is a knowledge graph that is multilingual by design and is published under legal terms that allow the broadest possible reuse \citep{vrandevcic2014Wikidata}. Information from Wikidata can be represented as semantic triples in the form of Resource Description Format (RDF) triples \citep{erxleben}. RDF triples are a standardized way to describe information. They are in the form \texttt{<subject, predicate, object>}. For example, the triple \texttt{ <dog, produced sound, bark>} indicates that a dog can make a barking sound. Because Wikidata is multilingual by design, IDs represent the elements of the triple that can be translated into a description in one of the many languages supported by Wikidata. The Wikidata RDF triple \texttt{<wd:Q144, wdt:P4733, wd:Q38681>} describes the previous example.\\ 


\section{Wikipedia subgraphs}
\label{secWikipediaSubgraphs}
\citet{Ponza2017} conducted a "study of all entity relatedness measures in recent literature based on Wikipedia as the knowledge graph." In their study, they devised a system for calculating a relatedness measure between two Wikipedia articles. The study found the  CoSimRank algorithm \citep{cosimrank} to be a good measure for calculating this. The disadvantage of this algorithm is that it does not perform well on large graphs like the Wikipedia article graph. To overcome the performance problem, a small subgraph is constructed consisting of articles close to the original articles. The CosSimRank algorithm uses this subgraph to perform its calculations \citep{cosimrank}. \\

The system devised by \citet{Ponza2017} is too complicated to be implemented for this thesis. It would have taken too much time and resources. For constructing the previously described subgraph, \citet{Ponza2017}, have analyzed several algorithms that use either the text of the article or its hyperlinks. The authors found that one of the best methods using the hyperlink structure of Wikipedia is the Milne\&Witten score described by \citet{Witten2008} or DeepWalk, described by \citet{deepwalk}. The Milne\&Witten score can be calculated easily and quickly without spending too much time. For these reasons, this score was chosen over the CoSimRank or Deepwalk algorithms to implement in this thesis. Formula \ref{formulaMilneWitten} contains the formula for calculating the Milne\&Witten score.\\

\begin{equation}
\captionsetup{justification=centering}
\label{formulaMilneWitten}
1 -\frac{ log( max( |A|, | B| )-log(| A \cap B|))}{log(|W|)-log(min\{|A|, |B|\})}
\end{equation}
\\ 
The formula \ref{formulaMilneWitten} calculates the Milne\&Witten score, where a and b are the two articles of interest, A, and B are the sets of all articles that link to a and b, respectively, and W is the entire Wikipedia.

\section{Training neural networks}
\label{training}
\label{secTrainingNeuralNetworks}
Neural networks need a dataset to be trained. To properly train a model and assure proper learning and performance, the dataset is split into three datasets: a training set, a validation set, and a test set (See figure \ref{imgTrainValidationTest}). Firstly, the basic training of the parameters of a model is done using the training set. Secondly, the model is trained several times to determine the optimal hyperparameter configuration. The validation set is used to find the most optimal configuration by selecting the configuration with the lowest error on the validation set. Lastly, the test set is used to determine the model's performance. This set cannot be the same as the validation set because the validation set was used to determine the configuration of the model and, therefore, can be biased. In research, 60\% of the dataset is often used as a training set, 20\% as a validation set, and 20\% as a test set; depending on the size of the dataset, these values can change.\\

\begin{figure}[h]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.75]{images/Datasetsplit.drawio.pdf}
\caption{A dataset is split into separate sets for training, validation and test purposes.}
\label{imgTrainValidationTest}\end{figure}


When dataset size is relatively low, it is essential to make optimal use of the dataset. As mentioned above, splitting it into three sets makes even fewer items available for training. K-Fold Cross-Validation is an often-used method to use all values for training without "losing" values for validation. With this method, the dataset is split into K folds, where one is used for validation, and K-1 folds are used for training. In the next iteration, another fold is used for validation, and the remaining folds are used for training. This process is repeated K-times until all folds are used for validation. The final result of the training is the average of the accuracy and F1 value of all iterations. Using K-Fold Cross-Validation, the relatively small number of training examples available in Wikisim and WiRe is used efficiently. This advantage compensates for the increased computer resources needed to train the neural network. A graphical representation of K-Fold Cross-Validation is depicted in figure \ref{imgKFold}.  
\\

\begin{figure}[h]
\centering
\captionsetup{justification=centering}
\includegraphics[scale=0.25]{images/kfold.png}
\caption{Schematic overview of K-fold Cross-Validation with K=5. (The image was copied from \\https://towardsdatascience.com /cross-validation-k-fold-vs-monte-carlo-e54df2fc179b).}
\label{imgKFold}\end{figure}\\

It is called overfitting when a neural network produces good results on the training but not the test set. This means the model learned too well from the training data but cannot perform well on the unseen data. Hence, the model is not able to generalize well. Overfitting typically occurs when:
\begin{itemize}
  \item The training data size is too small and does not contain enough data samples to accurately represent all possible input data values.
  \item The training dataset includes a significant amount of irrelevant information.
  \item Because of the high complexity of the neural network it learns on invalid data in the training data.
\end{itemize}\\

To mitigate overfitting the following (not extensive) list of options is available:
\begin{itemize}
\item \textbf{Pruning}: Decrease the number of features in the data, so less parameters are needed in the network.
\item \textbf{Regularization}: Add "noise" to the data by mathematical computations in order to make the network more rebust.
\item \textbf{Data augmentation}: Artificially create extra data based on the data in datasets.
\end{itemize}




